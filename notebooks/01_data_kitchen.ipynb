{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0a646f7",
   "metadata": {},
   "source": [
    "# üß™ Project Chimera 2.0 ‚Äî Phase 1: The Data Kitchen\n",
    "---\n",
    "**Objective:** Ingest massive, heterogeneous raw datasets (Tweets, 1-Min Crypto, Daily Macro) and fuse them into a single, clean, hourly-aligned Parquet file ready for feature engineering.\n",
    "\n",
    "**Scientific Alpha Logic:**\n",
    "> Markets are driven by a confluence of signals operating at different frequencies.\n",
    "> By aligning sentiment (social), price action (market microstructure), and macro-economic regime indicators onto a single hourly timeline, we create a unified information surface that no single data source can provide alone.\n",
    "\n",
    "**Output:** `chimera_master_dataset.parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2882a7",
   "metadata": {},
   "source": [
    "## Step 0 ‚Äî Environment Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d852ce1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:41:37.090599Z",
     "iopub.status.busy": "2026-02-18T09:41:37.090183Z",
     "iopub.status.idle": "2026-02-18T09:41:37.146858Z",
     "shell.execute_reply": "2026-02-18T09:41:37.145805Z",
     "shell.execute_reply.started": "2026-02-18T09:41:37.090569Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found 6 dataset folder(s) under /kaggle/input:\n",
      "   ‚Ä¢ datasets/eswaranmuthu/u-s-economic-vital-signs-25-years-of-macro-data\n",
      "   ‚Ä¢ datasets/kaushiksuresh147/bitcoin-tweets\n",
      "   ‚Ä¢ datasets/liiucbs/crypto-fear-and-greed-index\n",
      "   ‚Ä¢ datasets/novandraanugrah/bitcoin-historical-datasets-2018-2024\n",
      "   ‚Ä¢ datasets/organizations/federalreserve/real-trade-weighted-u.s.-dollar-index-collection\n",
      "   ‚Ä¢ datasets/rezanematpour/historical-s-and-p-500-gspc-index-data-19272025\n",
      "\n",
      "   BTC 1-Min   : /kaggle/input/datasets/novandraanugrah/bitcoin-historical-datasets-2018-2024/btc_15m_data_2018_to_2025.csv\n",
      "   Tweets      : /kaggle/input/datasets/kaushiksuresh147/bitcoin-tweets/Bitcoin_tweets.csv\n",
      "   S&P 500 dir : /kaggle/input/datasets/rezanematpour/historical-s-and-p-500-gspc-index-data-19272025\n",
      "   DXY dir     : /kaggle/input/datasets/organizations/federalreserve/real-trade-weighted-u.s.-dollar-index-collection\n",
      "   Fear & Greed: /kaggle/input/datasets/liiucbs/crypto-fear-and-greed-index/CryptoGreedFear.csv\n",
      "   Macro dir   : /kaggle/input/datasets/eswaranmuthu/u-s-economic-vital-signs-25-years-of-macro-data\n",
      "\n",
      "‚úÖ Environment ready.\n",
      "   NumPy  2.0.2  ‚Ä¢  Pandas 2.2.2\n",
      "   Output ‚Üí /kaggle/working/chimera_master_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 0: ENVIRONMENT SETUP & CONFIGURATION\n",
    "# ============================================================================\n",
    "# Install / verify dependencies (Kaggle kernels have most pre-installed)\n",
    "import subprocess, sys\n",
    "\n",
    "def _ensure_package(pkg_name, import_name=None):\n",
    "    \"\"\"Silently install a package if not already available.\"\"\"\n",
    "    import_name = import_name or pkg_name\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "    except ImportError:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg_name]\n",
    "        )\n",
    "\n",
    "_ensure_package(\"nltk\")\n",
    "_ensure_package(\"tqdm\")\n",
    "_ensure_package(\"pyarrow\")          # Parquet backend\n",
    "\n",
    "# --- Core Imports -----------------------------------------------------------\n",
    "import os, glob, re, warnings, gc, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 60)\n",
    "pd.set_option(\"display.float_format\", \"{:.4f}\".format)\n",
    "\n",
    "# --- NLTK VADER Lexicon (download once) --------------------------------------\n",
    "nltk.download(\"vader_lexicon\", quiet=True)\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# --- Paths -------------------------------------------------------------------\n",
    "# Kaggle mounts datasets in one of three layouts:\n",
    "#   A) /kaggle/input/<dataset-slug>/                        (classic)\n",
    "#   B) /kaggle/input/datasets/<dataset-slug>/               (newer)\n",
    "#   C) /kaggle/input/datasets/<author>/<dataset-slug>/      (API / newest)\n",
    "# We build a flat list of ALL leaf dataset directories regardless of nesting,\n",
    "# then search that list by keyword.\n",
    "\n",
    "_base = Path(\"/kaggle/input\")\n",
    "OUTPUT_DIR  = Path(\"/kaggle/working\")\n",
    "OUTPUT_FILE = OUTPUT_DIR / \"chimera_master_dataset.parquet\"\n",
    "\n",
    "# ---- Collect every dataset directory (leaf dirs that contain files) ----------\n",
    "def _collect_dataset_dirs(root: Path, max_depth: int = 4) -> list[Path]:\n",
    "    \"\"\"Walk up to *max_depth* levels and return dirs that contain files.\"\"\"\n",
    "    results = []\n",
    "    def _walk(p: Path, depth: int):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "        try:\n",
    "            children = sorted(p.iterdir())\n",
    "        except PermissionError:\n",
    "            return\n",
    "        has_files = any(c.is_file() for c in children)\n",
    "        if has_files:\n",
    "            results.append(p)\n",
    "        for c in children:\n",
    "            if c.is_dir():\n",
    "                _walk(c, depth + 1)\n",
    "    _walk(root, 0)\n",
    "    return results\n",
    "\n",
    "ALL_DATASET_DIRS = _collect_dataset_dirs(_base)\n",
    "\n",
    "print(f\"üìÇ Found {len(ALL_DATASET_DIRS)} dataset folder(s) under {_base}:\")\n",
    "for d in ALL_DATASET_DIRS:\n",
    "    print(f\"   ‚Ä¢ {d.relative_to(_base)}\")\n",
    "\n",
    "# ---- Helper: find the main CSV inside a directory ---------------------------\n",
    "def find_csv(directory: Path, hint: str = \"\") -> Path:\n",
    "    \"\"\"Robustly locate the primary CSV in *directory* (recursive).\"\"\"\n",
    "    candidates = sorted(directory.rglob(\"*.csv\"))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"No CSV found in {directory}\")\n",
    "    if hint:\n",
    "        matched = [c for c in candidates if hint.lower() in c.name.lower()]\n",
    "        if matched:\n",
    "            return matched[0]\n",
    "    return max(candidates, key=lambda p: p.stat().st_size)\n",
    "\n",
    "# ---- Auto-discover dataset directories by keyword ---------------------------\n",
    "def _find_dataset_dir(keyword: str, alt_keywords: list[str] = None) -> Path:\n",
    "    \"\"\"Search ALL_DATASET_DIRS for the first dir whose full path matches.\"\"\"\n",
    "    all_kw = [keyword] + (alt_keywords or [])\n",
    "    for kw in all_kw:\n",
    "        for d in ALL_DATASET_DIRS:\n",
    "            if kw.lower() in str(d).lower():\n",
    "                return d\n",
    "    raise FileNotFoundError(\n",
    "        f\"Cannot find dataset directory matching any of {all_kw}.\\n\"\n",
    "        f\"Available: {[str(d.relative_to(_base)) for d in ALL_DATASET_DIRS]}\"\n",
    "    )\n",
    "\n",
    "# ---- Dataset paths (resolved dynamically) -----------------------------------\n",
    "DIR_BTC        = _find_dataset_dir(\"bitcoin-historical\", [\"btc-usd\", \"binance\"])\n",
    "DIR_TWEETS     = _find_dataset_dir(\"bitcoin-tweet\",      [\"tweet\", \"twitter\"])\n",
    "DIR_SPX        = _find_dataset_dir(\"gspc\",               [\"sp500\", \"s-p-500\", \"s-and-p\"])\n",
    "DIR_DXY        = _find_dataset_dir(\"trade-weighted\",     [\"dollar-index\", \"dtwex\", \"dxy\"])\n",
    "DIR_FEAR_GREED = _find_dataset_dir(\"fear\",               [\"greed\", \"fgi\"])\n",
    "DIR_MACRO      = _find_dataset_dir(\"macro\",              [\"vital-signs\", \"economic\"])\n",
    "\n",
    "# Resolve exact file paths\n",
    "PATH_BTC_1M     = find_csv(DIR_BTC,        hint=\"1m\")\n",
    "PATH_TWEETS     = find_csv(DIR_TWEETS,     hint=\"tweet\")\n",
    "PATH_SPX_DIR    = DIR_SPX\n",
    "PATH_DXY_DIR    = DIR_DXY\n",
    "PATH_FEAR_GREED = find_csv(DIR_FEAR_GREED, hint=\"fear\")\n",
    "PATH_MACRO_DIR  = DIR_MACRO\n",
    "\n",
    "# Show what we resolved\n",
    "print(f\"\\n   BTC 1-Min   : {PATH_BTC_1M}\")\n",
    "print(f\"   Tweets      : {PATH_TWEETS}\")\n",
    "print(f\"   S&P 500 dir : {PATH_SPX_DIR}\")\n",
    "print(f\"   DXY dir     : {PATH_DXY_DIR}\")\n",
    "print(f\"   Fear & Greed: {PATH_FEAR_GREED}\")\n",
    "print(f\"   Macro dir   : {PATH_MACRO_DIR}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Environment ready.\")\n",
    "print(f\"   NumPy  {np.__version__}  ‚Ä¢  Pandas {pd.__version__}\")\n",
    "print(f\"   Output ‚Üí {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b99312",
   "metadata": {},
   "source": [
    "## Step 1 ‚Äî Sentiment Extraction (Memory-Safe Chunked Pipeline)\n",
    "\n",
    "**Why chunked?** The Bitcoin Tweets dataset contains millions of rows. Loading it in one shot will exceed Kaggle's RAM ceiling (‚âà30 GB).\n",
    "\n",
    "**Alpha Logic:** Social-media sentiment is a *leading* indicator of retail positioning. By aggregating VADER compound scores to the hourly grain, we capture the crowd's emotional state right before price moves ‚Äî giving the model a behavioural edge that pure price data cannot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e6312a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:41:37.148913Z",
     "iopub.status.busy": "2026-02-18T09:41:37.148588Z",
     "iopub.status.idle": "2026-02-18T10:03:13.885124Z",
     "shell.execute_reply": "2026-02-18T10:03:13.884239Z",
     "shell.execute_reply.started": "2026-02-18T09:41:37.148888Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† VADER lexicon updated with 10 crypto-specific terms\n",
      "üìñ Reading tweets in chunks of 100,000 from:\n",
      "   /kaggle/input/datasets/kaushiksuresh147/bitcoin-tweets/Bitcoin_tweets.csv\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17d9bdefd084f45bfb527c14c009094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentiment chunks: 0chunk [00:00, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Sentiment done ‚Äî 4,689,288 tweets ‚Üí 4,039 hourly rows  (1295s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_mean</th>\n",
       "      <th>sentiment_std</th>\n",
       "      <th>tweet_volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-05 10:00:00</th>\n",
       "      <td>0.3789</td>\n",
       "      <td>0.3420</td>\n",
       "      <td>11.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-05 11:00:00</th>\n",
       "      <td>0.1088</td>\n",
       "      <td>0.3642</td>\n",
       "      <td>88.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-05 12:00:00</th>\n",
       "      <td>0.2038</td>\n",
       "      <td>0.3425</td>\n",
       "      <td>139.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-05 13:00:00</th>\n",
       "      <td>0.1314</td>\n",
       "      <td>0.3307</td>\n",
       "      <td>131.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-05 14:00:00</th>\n",
       "      <td>0.1664</td>\n",
       "      <td>0.3388</td>\n",
       "      <td>160.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sentiment_mean  sentiment_std  tweet_volume\n",
       "timestamp                                                       \n",
       "2021-02-05 10:00:00          0.3789         0.3420       11.0000\n",
       "2021-02-05 11:00:00          0.1088         0.3642       88.0000\n",
       "2021-02-05 12:00:00          0.2038         0.3425      139.0000\n",
       "2021-02-05 13:00:00          0.1314         0.3307      131.0000\n",
       "2021-02-05 14:00:00          0.1664         0.3388      160.0000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: SENTIMENT EXTRACTION ‚Äî MEMORY-SAFE CHUNKED PIPELINE\n",
    "# ============================================================================\n",
    "# Scientific Alpha:\n",
    "#   VADER (Valence Aware Dictionary and sEntiment Reasoner) is specifically\n",
    "#   tuned for social-media text.  It handles slang, emojis, and punctuation\n",
    "#   emphasis (e.g., \"AMAZING!!!\") out of the box ‚Äî critical for crypto Twitter.\n",
    "# ============================================================================\n",
    "\n",
    "CHUNK_SIZE = 100_000   # rows per chunk ‚Äî keeps peak RAM well under 4 GB\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# --- Crypto-specific lexicon update ------------------------------------------\n",
    "# VADER's default lexicon was trained on general social-media text and misses\n",
    "# crypto-native slang entirely.  Without this update, terms like \"moon\",\n",
    "# \"rekt\", or \"hodl\" receive near-zero scores, flattening the sentiment signal.\n",
    "# These manually-curated scores align with empirical crypto-Twitter semantics.\n",
    "crypto_lexicon = {\n",
    "    # Positive / bullish terms  (scores pushed to extremes for sharper signal)\n",
    "    'moon': 5.0, 'bullish': 3.5, 'hodl': 2.0, 'ath': 4.0, 'btfd': 2.5,\n",
    "    # Negative / bearish terms\n",
    "    'rekt': -5.0, 'bearish': -3.5, 'rug': -5.0, 'fud': -3.0, 'dump': -3.0,\n",
    "}\n",
    "sia.lexicon.update(crypto_lexicon)\n",
    "print(f\"üß† VADER lexicon updated with {len(crypto_lexicon)} crypto-specific terms\")\n",
    "\n",
    "# --- Auto-detect timestamp column (used by Steps 3A‚Äì3D for macro data) -------\n",
    "def _detect_ts_col(cols):\n",
    "    \"\"\"Pick the best timestamp column from a list of column names.\"\"\"\n",
    "    for candidate in [\"timestamp\", \"date\", \"created_at\", \"datetime\", \"Timestamp\", \"Date\"]:\n",
    "        if candidate in cols:\n",
    "            return candidate\n",
    "    for c in cols:\n",
    "        if \"time\" in c.lower() or \"date\" in c.lower():\n",
    "            return c\n",
    "    raise KeyError(f\"Cannot auto-detect timestamp column from {list(cols)}\")\n",
    "\n",
    "# --- Text cleaning regex (compiled once for speed) ---------------------------\n",
    "RE_URL      = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "RE_USERNAME = re.compile(r\"@\\w+\")\n",
    "RE_HASHTAG  = re.compile(r\"#\")           # Remove the '#' symbol but KEEP the word\n",
    "RE_MULTI_WS = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_tweet(text: str) -> str:\n",
    "    \"\"\"Strip URLs, @mentions, '#' symbols.  KEEP emojis (VADER scores them).\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = RE_URL.sub(\"\", text)\n",
    "    text = RE_USERNAME.sub(\"\", text)\n",
    "    text = RE_HASHTAG.sub(\"\", text)       # '#Bitcoin' ‚Üí 'Bitcoin'\n",
    "    text = RE_MULTI_WS.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def vader_compound(text: str) -> float:\n",
    "    \"\"\"Return VADER compound score (‚Äì1 ‚Ä¶ +1).\"\"\"\n",
    "    return sia.polarity_scores(text)[\"compound\"]\n",
    "\n",
    "# --- Chunked processing loop ------------------------------------------------\n",
    "hourly_chunks: list[pd.DataFrame] = []\n",
    "total_tweets = 0\n",
    "\n",
    "print(f\"üìñ Reading tweets in chunks of {CHUNK_SIZE:,} from:\\n   {PATH_TWEETS}\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "reader = pd.read_csv(PATH_TWEETS, chunksize=CHUNK_SIZE, lineterminator=\"\\n\",\n",
    "                      on_bad_lines=\"skip\", engine=\"c\")\n",
    "\n",
    "for i, chunk in enumerate(tqdm(reader, desc=\"Sentiment chunks\", unit=\"chunk\")):\n",
    "\n",
    "    # ‚îÄ‚îÄ 1) Robust Date Parsing (FIX for all-zero sentiment bug) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    try:\n",
    "        # 1a. Explicitly target the 'date' column (format: \"YYYY-MM-DD HH:MM:SS\")\n",
    "        chunk['timestamp'] = pd.to_datetime(chunk['date'], errors='coerce')\n",
    "\n",
    "        # 1b. Strip timezone if present (safety net ‚Äî Binance data is tz-naive)\n",
    "        if pd.api.types.is_datetime64_any_dtype(chunk['timestamp']):\n",
    "            if chunk['timestamp'].dt.tz is not None:\n",
    "                chunk['timestamp'] = chunk['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "        # 1c. Floor to nearest hour (critical for merging with hourly market data)\n",
    "        chunk['hour'] = chunk['timestamp'].dt.floor('h')\n",
    "\n",
    "        # 1d. Drop rows where date parsing failed\n",
    "        chunk.dropna(subset=['hour'], inplace=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error parsing date in chunk {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 2) Detect the text column\n",
    "    text_col = None\n",
    "    for candidate in [\"text\", \"tweet\", \"content\", \"Tweet\", \"Text\"]:\n",
    "        if candidate in chunk.columns:\n",
    "            text_col = candidate\n",
    "            break\n",
    "    if text_col is None:\n",
    "        # Fallback: longest-average-string column (heuristic)\n",
    "        str_cols = chunk.select_dtypes(include=\"object\").columns\n",
    "        text_col = max(str_cols, key=lambda c: chunk[c].astype(str).str.len().mean())\n",
    "\n",
    "    # 3) Clean & score\n",
    "    chunk[\"clean_text\"]      = chunk[text_col].apply(clean_tweet)\n",
    "    chunk[\"vader_compound\"]  = chunk[\"clean_text\"].apply(vader_compound)\n",
    "\n",
    "    # 4) Aggregate to hourly grain IMMEDIATELY (memory-safe)\n",
    "    agg = (\n",
    "        chunk.groupby(\"hour\")\n",
    "        .agg(\n",
    "            sentiment_mean=(\"vader_compound\", \"mean\"),\n",
    "            sentiment_std =(\"vader_compound\", \"std\"),\n",
    "            tweet_volume  =(\"vader_compound\", \"size\"),\n",
    "        )\n",
    "    )\n",
    "    hourly_chunks.append(agg)\n",
    "    total_tweets += len(chunk)\n",
    "\n",
    "    # Free memory\n",
    "    del chunk, agg\n",
    "    gc.collect()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "# --- Combine all hourly aggregations ----------------------------------------\n",
    "# Because the same hour can span multiple chunks, we do a weighted re-agg.\n",
    "df_sentiment_raw = pd.concat(hourly_chunks)\n",
    "del hourly_chunks\n",
    "gc.collect()\n",
    "\n",
    "# Weighted mean sentiment across chunks for the same hour\n",
    "df_sentiment_hourly = (\n",
    "    df_sentiment_raw\n",
    "    .groupby(level=0)\n",
    "    .apply(\n",
    "        lambda g: pd.Series({\n",
    "            \"sentiment_mean\": np.average(g[\"sentiment_mean\"], weights=g[\"tweet_volume\"]),\n",
    "            \"sentiment_std\" : g[\"sentiment_std\"].mean(),       # approx pooled std\n",
    "            \"tweet_volume\"  : g[\"tweet_volume\"].sum(),\n",
    "        })\n",
    "    )\n",
    ")\n",
    "df_sentiment_hourly.index.name = \"timestamp\"\n",
    "\n",
    "del df_sentiment_raw\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n‚úÖ Sentiment done ‚Äî {total_tweets:,} tweets ‚Üí \"\n",
    "      f\"{len(df_sentiment_hourly):,} hourly rows  ({elapsed:.0f}s)\")\n",
    "df_sentiment_hourly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e95d8",
   "metadata": {},
   "source": [
    "## Step 2 ‚Äî Market Data Processing (The Backbone)\n",
    "\n",
    "**Alpha Logic:** 1-minute Binance candles are the highest-fidelity price signal available. Resampling to 1-Hour OHLCV produces the **master timeline** ‚Äî every other signal gets aligned to this index. The market dataframe is the \"spine\" of the final dataset; its index defines which hours exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8988b311",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:03:13.887038Z",
     "iopub.status.busy": "2026-02-18T10:03:13.886752Z",
     "iopub.status.idle": "2026-02-18T10:03:16.149827Z",
     "shell.execute_reply": "2026-02-18T10:03:16.148727Z",
     "shell.execute_reply.started": "2026-02-18T10:03:13.887014Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Loading BTC candles from:\n",
      "   /kaggle/input/datasets/novandraanugrah/bitcoin-historical-datasets-2018-2024/btc_15m_data_2018_to_2025.csv\n",
      "\n",
      "   Columns found: ['Open time', 'Open', 'High', 'Low', 'Close', 'Volume', 'Close time', 'Quote asset volume', 'Number of trades', 'Taker buy base asset volume', 'Taker buy quote asset volume', 'Ignore']\n",
      "   Raw rows: 284,589\n",
      "   First row sample:\n",
      "                     Open time       Open       High        Low      Close   Volume                   Close time  Quote asset volume  Number of trades  Taker buy base asset volume  Taker buy quote asset volume  Ignore\n",
      "0  2018-01-01 00:00:00.000000  13715.6500 13715.6500 13400.0100 13556.1500 123.6160  2018-01-01 00:14:59.999000         1675544.8866              1572                      63.2271                   857610.8270       0\n",
      "1  2018-01-01 00:15:00.000000  13533.7500 13550.8700 13402.0000 13521.1200  98.1364  2018-01-01 00:29:59.999000         1321756.8518              1461                      47.6864                   642281.1723       0\n",
      "\n",
      "   Detected mapping: {'timestamp': 'Open time', 'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'volume': 'Volume'}\n",
      "\n",
      "‚úÖ Market backbone ready ‚Äî 71,158 hourly candles\n",
      "   Range: 2018-01-01 00:00:00 ‚Üí 2026-02-17 23:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>13715.6500</td>\n",
       "      <td>13715.6500</td>\n",
       "      <td>13400.0100</td>\n",
       "      <td>13529.0100</td>\n",
       "      <td>443.3562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:00:00</th>\n",
       "      <td>13528.9900</td>\n",
       "      <td>13595.8900</td>\n",
       "      <td>13155.3800</td>\n",
       "      <td>13203.0600</td>\n",
       "      <td>383.6970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 02:00:00</th>\n",
       "      <td>13203.0000</td>\n",
       "      <td>13418.4300</td>\n",
       "      <td>13200.0000</td>\n",
       "      <td>13330.1800</td>\n",
       "      <td>429.0646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 03:00:00</th>\n",
       "      <td>13330.2600</td>\n",
       "      <td>13611.2700</td>\n",
       "      <td>13290.0000</td>\n",
       "      <td>13410.0300</td>\n",
       "      <td>420.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 04:00:00</th>\n",
       "      <td>13434.9800</td>\n",
       "      <td>13623.2900</td>\n",
       "      <td>13322.1500</td>\n",
       "      <td>13601.0100</td>\n",
       "      <td>340.8073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          open       high        low      close   volume\n",
       "timestamp                                                               \n",
       "2018-01-01 00:00:00 13715.6500 13715.6500 13400.0100 13529.0100 443.3562\n",
       "2018-01-01 01:00:00 13528.9900 13595.8900 13155.3800 13203.0600 383.6970\n",
       "2018-01-01 02:00:00 13203.0000 13418.4300 13200.0000 13330.1800 429.0646\n",
       "2018-01-01 03:00:00 13330.2600 13611.2700 13290.0000 13410.0300 420.0870\n",
       "2018-01-01 04:00:00 13434.9800 13623.2900 13322.1500 13601.0100 340.8073"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: MARKET DATA PROCESSING ‚Äî THE BACKBONE\n",
    "# ============================================================================\n",
    "# Scientific Alpha:\n",
    "#   The Binance BTC/USD dataset captures micro-structure moves.  By\n",
    "#   resampling to 1H OHLCV we preserve intra-hour range (High-Low)\n",
    "#   and volume dynamics ‚Äî both proven alpha signals in momentum literature.\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"üìñ Loading BTC candles from:\\n   {PATH_BTC_1M}\\n\")\n",
    "\n",
    "df_btc_raw = pd.read_csv(PATH_BTC_1M)\n",
    "\n",
    "print(f\"   Columns found: {list(df_btc_raw.columns)}\")\n",
    "print(f\"   Raw rows: {len(df_btc_raw):,}\")\n",
    "print(f\"   First row sample:\\n{df_btc_raw.head(2).to_string()}\\n\")\n",
    "\n",
    "# --- Auto-detect columns ----------------------------------------------------\n",
    "col_map = {}\n",
    "for col in df_btc_raw.columns:\n",
    "    cl = col.strip().lower()\n",
    "    if cl in (\"timestamp\", \"date\", \"datetime\", \"time\", \"open_time\", \"open time\"):\n",
    "        col_map[\"timestamp\"] = col\n",
    "    elif cl == \"open\":\n",
    "        col_map[\"open\"] = col\n",
    "    elif cl == \"high\":\n",
    "        col_map[\"high\"] = col\n",
    "    elif cl == \"low\":\n",
    "        col_map[\"low\"] = col\n",
    "    elif cl == \"close\":\n",
    "        col_map[\"close\"] = col\n",
    "    elif cl in (\"volume\", \"vol\", \"volume_(btc)\", \"volume btc\"):\n",
    "        col_map[\"volume\"] = col\n",
    "\n",
    "# If no timestamp column found, check if the index is datetime-like, or\n",
    "# if there's an unnamed first column that looks like dates\n",
    "if \"timestamp\" not in col_map:\n",
    "    # Check for 'Unnamed: 0' or first column being a date\n",
    "    first_col = df_btc_raw.columns[0]\n",
    "    sample_val = str(df_btc_raw[first_col].dropna().iloc[0])\n",
    "    # Test if it parses as a date\n",
    "    try:\n",
    "        pd.to_datetime(sample_val)\n",
    "        col_map[\"timestamp\"] = first_col\n",
    "        print(f\"   ‚Üí Auto-detected '{first_col}' as timestamp column\")\n",
    "    except (ValueError, TypeError):\n",
    "        # Last resort: try to parse the index\n",
    "        try:\n",
    "            pd.to_datetime(df_btc_raw.index[:5])\n",
    "            df_btc_raw = df_btc_raw.reset_index()\n",
    "            col_map[\"timestamp\"] = df_btc_raw.columns[0]\n",
    "            print(f\"   ‚Üí Using index as timestamp column\")\n",
    "        except (ValueError, TypeError):\n",
    "            raise KeyError(\n",
    "                f\"Cannot find timestamp column. Columns: {list(df_btc_raw.columns)}\\n\"\n",
    "                f\"First row: {df_btc_raw.iloc[0].to_dict()}\"\n",
    "            )\n",
    "\n",
    "print(f\"   Detected mapping: {col_map}\")\n",
    "\n",
    "# --- Parse & set datetime index ----------------------------------------------\n",
    "ts_col = col_map[\"timestamp\"]\n",
    "\n",
    "sample = df_btc_raw[ts_col].dropna().iloc[0]\n",
    "if isinstance(sample, (int, float, np.integer, np.floating)) or str(sample).isdigit():\n",
    "    val = int(float(str(sample)))\n",
    "    unit = \"ms\" if val > 1e12 else \"s\"\n",
    "    df_btc_raw[\"timestamp\"] = pd.to_datetime(df_btc_raw[ts_col].astype(float), unit=unit, utc=True)\n",
    "else:\n",
    "    df_btc_raw[\"timestamp\"] = pd.to_datetime(df_btc_raw[ts_col], utc=True, errors=\"coerce\")\n",
    "\n",
    "df_btc_raw.set_index(\"timestamp\", inplace=True)\n",
    "df_btc_raw.sort_index(inplace=True)\n",
    "\n",
    "# --- Rename to canonical names -----------------------------------------------\n",
    "rename = {col_map[k]: k for k in (\"open\", \"high\", \"low\", \"close\", \"volume\") if k in col_map}\n",
    "df_btc_raw.rename(columns=rename, inplace=True)\n",
    "\n",
    "# --- Resample to 1-Hour OHLCV -----------------------------------------------\n",
    "df_market = df_btc_raw[[\"open\", \"high\", \"low\", \"close\", \"volume\"]].resample(\"1h\").agg({\n",
    "    \"open\":   \"first\",\n",
    "    \"high\":   \"max\",\n",
    "    \"low\":    \"min\",\n",
    "    \"close\":  \"last\",\n",
    "    \"volume\": \"sum\",\n",
    "}).dropna()\n",
    "\n",
    "# Make index timezone-naive for uniform merging later\n",
    "if df_market.index.tz is not None:\n",
    "    df_market.index = df_market.index.tz_localize(None)\n",
    "df_market.index.name = \"timestamp\"\n",
    "\n",
    "del df_btc_raw\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n‚úÖ Market backbone ready ‚Äî {len(df_market):,} hourly candles\")\n",
    "print(f\"   Range: {df_market.index.min()} ‚Üí {df_market.index.max()}\")\n",
    "df_market.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4636252c",
   "metadata": {},
   "source": [
    "## Step 3 ‚Äî Macro & Auxiliary Data Fusion\n",
    "\n",
    "**Alpha Logic:** Cross-asset correlation regimes shift over time. Including S&P 500, DXY, Fear & Greed, and CPI/Interest-Rate data lets the model learn *when* BTC behaves as a risk-on proxy vs. a dollar-hedge vs. an uncorrelated asset ‚Äî a structural edge for regime detection.\n",
    "\n",
    "**Forward-Fill Rule:** Daily/monthly data is forward-filled to hourly so that every row reflects *only information available at that point in time*. NO backward fill ‚Äî that would leak future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b904305",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:03:16.151787Z",
     "iopub.status.busy": "2026-02-18T10:03:16.151245Z",
     "iopub.status.idle": "2026-02-18T10:03:16.390640Z",
     "shell.execute_reply": "2026-02-18T10:03:16.389628Z",
     "shell.execute_reply.started": "2026-02-18T10:03:16.151755Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Loading S&P 500 data ...\n",
      "   Found: SP500.csv\n",
      "‚úÖ S&P 500 ‚Äî 24,532 daily ‚Üí 856,129 hourly rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3A: S&P 500 (DAILY ‚Üí HOURLY)\n",
    "# ============================================================================\n",
    "# Scientific Alpha:\n",
    "#   S&P 500 is the global risk barometer.  BTC's correlation with equities\n",
    "#   has increased post-2020 as institutional allocators treat it as a risk\n",
    "#   asset.  Including SPX lets the model capture risk-on / risk-off regime\n",
    "#   shifts that precede crypto moves.\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üìñ Loading S&P 500 data ...\")\n",
    "\n",
    "try:\n",
    "    spx_path = find_csv(PATH_SPX_DIR, hint=\"sp500\")\n",
    "except FileNotFoundError:\n",
    "    spx_path = find_csv(PATH_SPX_DIR)  # fallback: largest CSV\n",
    "\n",
    "print(f\"   Found: {spx_path.name}\")\n",
    "\n",
    "df_spx_raw = pd.read_csv(spx_path)\n",
    "\n",
    "# Auto-detect date & close columns\n",
    "spx_date_col  = _detect_ts_col(df_spx_raw.columns)\n",
    "spx_close_col = None\n",
    "for c in df_spx_raw.columns:\n",
    "    if \"close\" in c.lower() or \"adj\" in c.lower() or \"price\" in c.lower():\n",
    "        spx_close_col = c\n",
    "        break\n",
    "if spx_close_col is None:\n",
    "    num_cols = df_spx_raw.select_dtypes(include=\"number\").columns\n",
    "    spx_close_col = [c for c in num_cols if \"vol\" not in c.lower()][0]\n",
    "\n",
    "df_spx = pd.DataFrame({\n",
    "    \"timestamp\": pd.to_datetime(df_spx_raw[spx_date_col], errors=\"coerce\"),\n",
    "    \"spx_close\": pd.to_numeric(df_spx_raw[spx_close_col], errors=\"coerce\"),\n",
    "}).dropna()\n",
    "\n",
    "df_spx.set_index(\"timestamp\", inplace=True)\n",
    "df_spx.sort_index(inplace=True)\n",
    "\n",
    "if df_spx.index.tz is not None:\n",
    "    df_spx.index = df_spx.index.tz_localize(None)\n",
    "\n",
    "# Resample Daily ‚Üí Hourly with forward-fill (NO backward fill = no leakage)\n",
    "df_spx_hourly = df_spx.resample(\"1h\").ffill()\n",
    "\n",
    "print(f\"‚úÖ S&P 500 ‚Äî {len(df_spx):,} daily ‚Üí {len(df_spx_hourly):,} hourly rows\")\n",
    "del df_spx_raw, df_spx\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77f84458",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:03:16.392804Z",
     "iopub.status.busy": "2026-02-18T10:03:16.392475Z",
     "iopub.status.idle": "2026-02-18T10:03:16.540416Z",
     "shell.execute_reply": "2026-02-18T10:03:16.539483Z",
     "shell.execute_reply.started": "2026-02-18T10:03:16.392778Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Loading DXY Dollar Index ...\n",
      "   Found: DTWEXB.csv\n",
      "‚úÖ DXY ‚Äî 6,317 daily ‚Üí 218,617 hourly rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3B: DXY ‚Äî U.S. DOLLAR INDEX (DAILY ‚Üí HOURLY)\n",
    "# ============================================================================\n",
    "# Scientific Alpha:\n",
    "#   BTC is priced in USD.  A strengthening dollar (rising DXY) typically\n",
    "#   pressures all risk assets.  The DXY captures global liquidity tightening\n",
    "#   signals earlier than crypto-native indicators.\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üìñ Loading DXY Dollar Index ...\")\n",
    "\n",
    "# The dataset contains multiple FRED series; pick DTWEXB (Broad) as the\n",
    "# single best proxy for overall dollar strength.\n",
    "try:\n",
    "    dxy_path = find_csv(PATH_DXY_DIR, hint=\"dtwexb\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        dxy_path = find_csv(PATH_DXY_DIR, hint=\"dollar\")\n",
    "    except FileNotFoundError:\n",
    "        dxy_path = find_csv(PATH_DXY_DIR)\n",
    "\n",
    "print(f\"   Found: {dxy_path.name}\")\n",
    "\n",
    "df_dxy_raw = pd.read_csv(dxy_path)\n",
    "\n",
    "dxy_date_col = _detect_ts_col(df_dxy_raw.columns)\n",
    "\n",
    "# Find the value column (could be 'DTWEXBGS', 'Value', 'Close', etc.)\n",
    "dxy_val_col = None\n",
    "for c in df_dxy_raw.columns:\n",
    "    cl = c.strip().lower()\n",
    "    if cl in (\"value\", \"close\", \"price\", \"dtwexbgs\", \"dtwexb\", \"dtwexm\", \"index\"):\n",
    "        dxy_val_col = c\n",
    "        break\n",
    "if dxy_val_col is None:\n",
    "    num_cols = df_dxy_raw.select_dtypes(include=\"number\").columns.tolist()\n",
    "    num_cols = [c for c in num_cols if \"year\" not in c.lower() and \"id\" not in c.lower()]\n",
    "    if num_cols:\n",
    "        dxy_val_col = num_cols[0]\n",
    "    else:\n",
    "        non_date = [c for c in df_dxy_raw.columns if c != dxy_date_col]\n",
    "        dxy_val_col = non_date[0]\n",
    "\n",
    "df_dxy = pd.DataFrame({\n",
    "    \"timestamp\": pd.to_datetime(df_dxy_raw[dxy_date_col], errors=\"coerce\"),\n",
    "    \"dxy_close\": pd.to_numeric(df_dxy_raw[dxy_val_col], errors=\"coerce\"),\n",
    "}).dropna()\n",
    "\n",
    "df_dxy.set_index(\"timestamp\", inplace=True)\n",
    "df_dxy.sort_index(inplace=True)\n",
    "if df_dxy.index.tz is not None:\n",
    "    df_dxy.index = df_dxy.index.tz_localize(None)\n",
    "\n",
    "df_dxy_hourly = df_dxy.resample(\"1h\").ffill()\n",
    "\n",
    "print(f\"‚úÖ DXY ‚Äî {len(df_dxy):,} daily ‚Üí {len(df_dxy_hourly):,} hourly rows\")\n",
    "del df_dxy_raw, df_dxy\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e628084",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:03:16.541984Z",
     "iopub.status.busy": "2026-02-18T10:03:16.541622Z",
     "iopub.status.idle": "2026-02-18T10:03:16.663733Z",
     "shell.execute_reply": "2026-02-18T10:03:16.662760Z",
     "shell.execute_reply.started": "2026-02-18T10:03:16.541950Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Loading Fear & Greed Index ...\n",
      "   Path: /kaggle/input/datasets/liiucbs/crypto-fear-and-greed-index/CryptoGreedFear.csv\n",
      "‚úÖ Fear & Greed ‚Äî 1,257 daily ‚Üí 67,849 hourly rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3C: FEAR & GREED INDEX (DAILY ‚Üí HOURLY)\n",
    "# ============================================================================\n",
    "# Scientific Alpha:\n",
    "#   The Crypto Fear & Greed Index is a composite scoring retail emotion\n",
    "#   (0 = Extreme Fear, 100 = Extreme Greed).  It's a proven contrarian\n",
    "#   signal: extreme fear often marks local bottoms; extreme greed, tops.\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üìñ Loading Fear & Greed Index ...\")\n",
    "print(f\"   Path: {PATH_FEAR_GREED}\")\n",
    "\n",
    "df_fg_raw = pd.read_csv(PATH_FEAR_GREED)\n",
    "\n",
    "# Auto-detect columns\n",
    "fg_date_col = _detect_ts_col(df_fg_raw.columns)\n",
    "\n",
    "fg_val_col = None\n",
    "for c in df_fg_raw.columns:\n",
    "    cl = c.strip().lower()\n",
    "    if \"value\" in cl or \"greed\" in cl or \"fear\" in cl or \"fgi\" in cl or \"index\" in cl:\n",
    "        if \"class\" not in cl:   # skip classification columns\n",
    "            fg_val_col = c\n",
    "            break\n",
    "if fg_val_col is None:\n",
    "    fg_val_col = df_fg_raw.select_dtypes(include=\"number\").columns[0]\n",
    "\n",
    "df_fg = pd.DataFrame({\n",
    "    \"timestamp\":  pd.to_datetime(df_fg_raw[fg_date_col], errors=\"coerce\"),\n",
    "    \"fear_greed\": pd.to_numeric(df_fg_raw[fg_val_col], errors=\"coerce\"),\n",
    "}).dropna()\n",
    "\n",
    "df_fg.set_index(\"timestamp\", inplace=True)\n",
    "df_fg.sort_index(inplace=True)\n",
    "if df_fg.index.tz is not None:\n",
    "    df_fg.index = df_fg.index.tz_localize(None)\n",
    "\n",
    "df_fg_hourly = df_fg.resample(\"1h\").ffill()\n",
    "\n",
    "print(f\"‚úÖ Fear & Greed ‚Äî {len(df_fg):,} daily ‚Üí {len(df_fg_hourly):,} hourly rows\")\n",
    "del df_fg_raw, df_fg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd470d21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:03:16.665610Z",
     "iopub.status.busy": "2026-02-18T10:03:16.665166Z",
     "iopub.status.idle": "2026-02-18T10:03:16.922118Z",
     "shell.execute_reply": "2026-02-18T10:03:16.921269Z",
     "shell.execute_reply.started": "2026-02-18T10:03:16.665578Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Loading U.S. Macro data ...\n",
      "   Scanning: /kaggle/input/datasets/eswaranmuthu/u-s-economic-vital-signs-25-years-of-macro-data\n",
      "\n",
      "   Found 1 CSV(s):\n",
      "     ‚Ä¢ macro_data_25yrs.csv  (111 KB)\n",
      "\n",
      "   Using combined file: macro_data_25yrs.csv\n",
      "   ‚Üí Extracted CPI from column 'CPI'\n",
      "   ‚Üí Extracted Interest Rate from column 'Fed Funds Rate'\n",
      "\n",
      "‚úÖ Macro data fused ‚Äî 63,433 hourly rows, cols: ['cpi', 'interest_rate']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3D: U.S. MACRO DATA ‚Äî CPI & INTEREST RATES (MONTHLY ‚Üí HOURLY)\n",
    "# ============================================================================\n",
    "# Scientific Alpha:\n",
    "#   CPI (inflation) and the Federal Funds Rate define the *monetary regime*.\n",
    "#   Rising rates / hot CPI ‚Üí risk-off.  Falling rates / cooling CPI ‚Üí\n",
    "#   risk-on.  These regime shifts drive multi-month trends in BTC price.\n",
    "#   By forward-filling monthly data to hourly, every row knows the\n",
    "#   *current* economic backdrop without any future leakage.\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üìñ Loading U.S. Macro data ...\")\n",
    "print(f\"   Scanning: {PATH_MACRO_DIR}\\n\")\n",
    "\n",
    "# --- Robust file discovery using glob ----------------------------------------\n",
    "macro_files = sorted(PATH_MACRO_DIR.glob(\"*.csv\"))\n",
    "print(f\"   Found {len(macro_files)} CSV(s):\")\n",
    "for f in macro_files:\n",
    "    print(f\"     ‚Ä¢ {f.name}  ({f.stat().st_size / 1024:.0f} KB)\")\n",
    "\n",
    "# Strategy: try to find CPI and Interest Rate files by keyword matching.\n",
    "# If the dataset is a single combined file, we load it and pick columns.\n",
    "\n",
    "def _find_macro_file(files, keywords):\n",
    "    \"\"\"Return the first file whose name matches any keyword.\"\"\"\n",
    "    for f in files:\n",
    "        name_lower = f.name.lower()\n",
    "        if any(kw in name_lower for kw in keywords):\n",
    "            return f\n",
    "    return None\n",
    "\n",
    "cpi_file  = _find_macro_file(macro_files, [\"cpi\", \"inflation\", \"consumer_price\", \"consumer price\"])\n",
    "rate_file = _find_macro_file(macro_files, [\"interest\", \"rate\", \"fed\", \"funds\", \"fedfunds\"])\n",
    "\n",
    "macro_dfs = []\n",
    "\n",
    "# --- Helper to load & resample a single macro series ------------------------\n",
    "def load_macro_series(filepath, value_hint, col_name):\n",
    "    \"\"\"Load a macro CSV, extract one numeric series, resample to hourly ffill.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    date_col = _detect_ts_col(df.columns)\n",
    "\n",
    "    # Find value column\n",
    "    val_col = None\n",
    "    for c in df.columns:\n",
    "        if any(h in c.lower() for h in value_hint):\n",
    "            val_col = c\n",
    "            break\n",
    "    if val_col is None:\n",
    "        num_cols = [c for c in df.select_dtypes(include=\"number\").columns\n",
    "                    if \"year\" not in c.lower() and \"id\" not in c.lower()]\n",
    "        val_col = num_cols[0] if num_cols else df.columns[-1]\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"timestamp\": pd.to_datetime(df[date_col], errors=\"coerce\"),\n",
    "        col_name:    pd.to_numeric(df[val_col], errors=\"coerce\"),\n",
    "    }).dropna()\n",
    "    out.set_index(\"timestamp\", inplace=True)\n",
    "    out.sort_index(inplace=True)\n",
    "    if out.index.tz is not None:\n",
    "        out.index = out.index.tz_localize(None)\n",
    "    return out.resample(\"1h\").ffill()\n",
    "\n",
    "# --- Case A: separate CPI & Rate files found --------------------------------\n",
    "if cpi_file and rate_file:\n",
    "    df_cpi_hourly  = load_macro_series(cpi_file,  [\"cpi\", \"value\", \"index\"], \"cpi\")\n",
    "    df_rate_hourly = load_macro_series(rate_file,  [\"rate\", \"value\", \"interest\", \"fedfunds\"], \"interest_rate\")\n",
    "    macro_dfs = [df_cpi_hourly, df_rate_hourly]\n",
    "    print(f\"\\n   CPI  file : {cpi_file.name} ‚Üí {len(df_cpi_hourly):,} hourly rows\")\n",
    "    print(f\"   Rate file : {rate_file.name} ‚Üí {len(df_rate_hourly):,} hourly rows\")\n",
    "\n",
    "# --- Case B: single combined file -------------------------------------------\n",
    "elif len(macro_files) >= 1:\n",
    "    combined_path = macro_files[0] if len(macro_files) == 1 else max(macro_files, key=lambda p: p.stat().st_size)\n",
    "    print(f\"\\n   Using combined file: {combined_path.name}\")\n",
    "    df_macro_raw = pd.read_csv(combined_path)\n",
    "    date_col = _detect_ts_col(df_macro_raw.columns)\n",
    "\n",
    "    # Try to extract CPI column\n",
    "    for c in df_macro_raw.columns:\n",
    "        if \"cpi\" in c.lower() or \"inflation\" in c.lower() or \"consumer\" in c.lower():\n",
    "            tmp = pd.DataFrame({\n",
    "                \"timestamp\": pd.to_datetime(df_macro_raw[date_col], errors=\"coerce\"),\n",
    "                \"cpi\": pd.to_numeric(df_macro_raw[c], errors=\"coerce\"),\n",
    "            }).dropna().set_index(\"timestamp\").sort_index()\n",
    "            if tmp.index.tz is not None:\n",
    "                tmp.index = tmp.index.tz_localize(None)\n",
    "            macro_dfs.append(tmp.resample(\"1h\").ffill())\n",
    "            print(f\"   ‚Üí Extracted CPI from column '{c}'\")\n",
    "            break\n",
    "\n",
    "    # Try to extract Interest Rate column\n",
    "    for c in df_macro_raw.columns:\n",
    "        if any(kw in c.lower() for kw in [\"rate\", \"interest\", \"fed\", \"funds\"]):\n",
    "            tmp = pd.DataFrame({\n",
    "                \"timestamp\": pd.to_datetime(df_macro_raw[date_col], errors=\"coerce\"),\n",
    "                \"interest_rate\": pd.to_numeric(df_macro_raw[c], errors=\"coerce\"),\n",
    "            }).dropna().set_index(\"timestamp\").sort_index()\n",
    "            if tmp.index.tz is not None:\n",
    "                tmp.index = tmp.index.tz_localize(None)\n",
    "            macro_dfs.append(tmp.resample(\"1h\").ffill())\n",
    "            print(f\"   ‚Üí Extracted Interest Rate from column '{c}'\")\n",
    "            break\n",
    "\n",
    "    # If no keyword matched, just take the first two numeric columns\n",
    "    if not macro_dfs:\n",
    "        num_cols = [c for c in df_macro_raw.select_dtypes(include=\"number\").columns\n",
    "                    if \"year\" not in c.lower() and \"id\" not in c.lower()]\n",
    "        for i, c in enumerate(num_cols[:2]):\n",
    "            name = \"cpi\" if i == 0 else \"interest_rate\"\n",
    "            tmp = pd.DataFrame({\n",
    "                \"timestamp\": pd.to_datetime(df_macro_raw[date_col], errors=\"coerce\"),\n",
    "                name: pd.to_numeric(df_macro_raw[c], errors=\"coerce\"),\n",
    "            }).dropna().set_index(\"timestamp\").sort_index()\n",
    "            if tmp.index.tz is not None:\n",
    "                tmp.index = tmp.index.tz_localize(None)\n",
    "            macro_dfs.append(tmp.resample(\"1h\").ffill())\n",
    "            print(f\"   ‚Üí Extracted '{name}' from column '{c}'\")\n",
    "\n",
    "    del df_macro_raw\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No macro files found ‚Äî skipping CPI/Rate columns.\")\n",
    "\n",
    "# --- Merge macro sub-dataframes into one -------------------------------------\n",
    "if macro_dfs:\n",
    "    df_macro_hourly = macro_dfs[0]\n",
    "    for extra in macro_dfs[1:]:\n",
    "        df_macro_hourly = df_macro_hourly.join(extra, how=\"outer\")\n",
    "    df_macro_hourly.ffill(inplace=True)\n",
    "    df_macro_hourly.index.name = \"timestamp\"\n",
    "    print(f\"\\n‚úÖ Macro data fused ‚Äî {len(df_macro_hourly):,} hourly rows, \"\n",
    "          f\"cols: {list(df_macro_hourly.columns)}\")\n",
    "else:\n",
    "    df_macro_hourly = pd.DataFrame()\n",
    "    print(\"‚ö†Ô∏è  No macro data available.\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4808c30",
   "metadata": {},
   "source": [
    "## Step 4 ‚Äî The Grand Merge\n",
    "\n",
    "**Strategy:** Left-join everything onto `df_market` (the BTC hourly backbone). This guarantees:\n",
    "1. Every row has valid OHLCV data (no phantom hours).\n",
    "2. Auxiliary signals are present only where BTC was trading.\n",
    "3. Sentiment NaNs ‚Üí 0 (no tweets = neutral assumption). Macro NaNs ‚Üí ffill (last known value persists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe42a485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:03:16.924141Z",
     "iopub.status.busy": "2026-02-18T10:03:16.923249Z",
     "iopub.status.idle": "2026-02-18T10:03:17.117740Z",
     "shell.execute_reply": "2026-02-18T10:03:17.116785Z",
     "shell.execute_reply.started": "2026-02-18T10:03:16.924108Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Merging all signals onto the market backbone ...\n",
      "\n",
      "   ‚úì Sentiment merged ‚Äî matched 4,037 hours\n",
      "   ‚úì S&P 500   merged ‚Äî 67,007 non-null hours\n",
      "   ‚úì DXY       merged ‚Äî 16,975 non-null hours\n",
      "   ‚úì Fear/Greed merged ‚Äî 67,727 non-null hours\n",
      "   ‚úì cpi            merged ‚Äî 63,345 non-null hours\n",
      "   ‚úì interest_rate  merged ‚Äî 63,345 non-null hours\n",
      "\n",
      "   üõ°Ô∏è  CPI shifted by +360 hours (15-day reporting lag)\n",
      "   üõ°Ô∏è  Interest Rate shifted by +360 hours (15-day reporting lag)\n",
      "\n",
      "   ‚úÇÔ∏è  Golden Window (threshold > 10):\n",
      "       Cropped 27,046 rows before 2021-02-05 10:00:00\n",
      "       Dataset now starts at: 2021-02-05 10:00:00\n",
      "\n",
      "   üßπ Dropna on ['close', 'spx_close', 'cpi', 'interest_rate']:\n",
      "       Before: 44,112 rows\n",
      "       After:  44,112 rows\n",
      "       Removed: 0 rows\n",
      "\n",
      "‚úÖ Grand Merge complete!\n",
      "   Shape: (44112, 13)\n",
      "   Memory: 2.6 MB\n",
      "   Date range: 2021-02-05 10:00:00 ‚Üí 2026-02-17 23:00:00\n",
      "   Columns: ['open', 'high', 'low', 'close', 'volume', 'sentiment_mean', 'sentiment_std', 'tweet_volume', 'spx_close', 'dxy_close', 'fear_greed', 'cpi', 'interest_rate']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: THE GRAND MERGE\n",
    "# ============================================================================\n",
    "# Scientific Alpha:\n",
    "#   By left-joining on the market backbone we ensure temporal consistency.\n",
    "#   Every row is anchored to a real BTC trading hour.  No synthetic hours,\n",
    "#   no forward-looking data.  This is the foundation of a leak-free dataset.\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîó Merging all signals onto the market backbone ...\\n\")\n",
    "\n",
    "df_final = df_market.copy()\n",
    "merge_report = {\"market\": len(df_final)}\n",
    "\n",
    "# --- 1) Sentiment (hourly) --------------------------------------------------\n",
    "if df_sentiment_hourly.index.tz is not None:\n",
    "    df_sentiment_hourly.index = df_sentiment_hourly.index.tz_localize(None)\n",
    "\n",
    "df_final = df_final.join(df_sentiment_hourly, how=\"left\")\n",
    "df_final[\"sentiment_mean\"].fillna(0, inplace=True)\n",
    "df_final[\"sentiment_std\"].fillna(0, inplace=True)\n",
    "df_final[\"tweet_volume\"].fillna(0, inplace=True)\n",
    "merge_report[\"sentiment\"] = df_sentiment_hourly.index.isin(df_final.index).sum()\n",
    "print(f\"   ‚úì Sentiment merged ‚Äî matched {merge_report['sentiment']:,} hours\")\n",
    "\n",
    "# --- 2) S&P 500 (hourly ffilled) --------------------------------------------\n",
    "df_final = df_final.join(df_spx_hourly, how=\"left\")\n",
    "merge_report[\"spx\"] = df_final[\"spx_close\"].notna().sum()\n",
    "print(f\"   ‚úì S&P 500   merged ‚Äî {merge_report['spx']:,} non-null hours\")\n",
    "\n",
    "# --- 3) DXY (hourly ffilled) ------------------------------------------------\n",
    "df_final = df_final.join(df_dxy_hourly, how=\"left\")\n",
    "merge_report[\"dxy\"] = df_final[\"dxy_close\"].notna().sum()\n",
    "print(f\"   ‚úì DXY       merged ‚Äî {merge_report['dxy']:,} non-null hours\")\n",
    "\n",
    "# --- 4) Fear & Greed (hourly ffilled) ---------------------------------------\n",
    "df_final = df_final.join(df_fg_hourly, how=\"left\")\n",
    "merge_report[\"fear_greed\"] = df_final[\"fear_greed\"].notna().sum()\n",
    "print(f\"   ‚úì Fear/Greed merged ‚Äî {merge_report['fear_greed']:,} non-null hours\")\n",
    "\n",
    "# --- 5) Macro (CPI + Interest Rate) -----------------------------------------\n",
    "if not df_macro_hourly.empty:\n",
    "    df_final = df_final.join(df_macro_hourly, how=\"left\")\n",
    "    for col in df_macro_hourly.columns:\n",
    "        merge_report[col] = df_final[col].notna().sum()\n",
    "        print(f\"   ‚úì {col:<14} merged ‚Äî {merge_report[col]:,} non-null hours\")\n",
    "\n",
    "# --- Handling residual NaNs --------------------------------------------------\n",
    "macro_cols = [\"spx_close\", \"dxy_close\", \"fear_greed\"]\n",
    "if \"cpi\" in df_final.columns:\n",
    "    macro_cols.append(\"cpi\")\n",
    "if \"interest_rate\" in df_final.columns:\n",
    "    macro_cols.append(\"interest_rate\")\n",
    "\n",
    "df_final[macro_cols] = df_final[macro_cols].ffill()\n",
    "\n",
    "# --- Anti-Data-Leakage: Macro Reporting Lag Shift ----------------------------\n",
    "# CPI and Interest Rate are published with a ~15-day lag after the reporting\n",
    "# period ends.  Without this shift, the model \"sees\" macro data 2-3 weeks\n",
    "# before it was publicly available ‚Äî a severe form of look-ahead bias that\n",
    "# inflates backtest performance but fails catastrophically in live trading.\n",
    "# We shift by 15 days √ó 24 hours = 360 hourly rows.\n",
    "MACRO_LAG_HOURS = 15 * 24  # 15-day reporting lag in hourly resolution\n",
    "\n",
    "if \"cpi\" in df_final.columns:\n",
    "    df_final[\"cpi\"] = df_final[\"cpi\"].shift(MACRO_LAG_HOURS)\n",
    "    print(f\"\\n   üõ°Ô∏è  CPI shifted by +{MACRO_LAG_HOURS} hours (15-day reporting lag)\")\n",
    "\n",
    "if \"interest_rate\" in df_final.columns:\n",
    "    df_final[\"interest_rate\"] = df_final[\"interest_rate\"].shift(MACRO_LAG_HOURS)\n",
    "    print(f\"   üõ°Ô∏è  Interest Rate shifted by +{MACRO_LAG_HOURS} hours (15-day reporting lag)\")\n",
    "\n",
    "# --- Golden Window Synchronization ------------------------------------------\n",
    "# Price data starts in 2018 but tweet data only begins Feb 2021.  The ~3 years\n",
    "# of sentiment=0 rows carry no informational value and introduce a structural\n",
    "# bias: the model learns that \"zero sentiment\" is the norm, diluting the\n",
    "# predictive power of real sentiment signals.\n",
    "#\n",
    "# FIX: Using threshold > 10 instead of > 0.  Sparse noise tweets (volume 1-10)\n",
    "# from bot/scraper artefacts can appear years before the real tweet dataset\n",
    "# starts, causing the crop to fail silently.  A threshold of 10 ensures we\n",
    "# anchor on the first hour with *meaningful* social-media coverage.\n",
    "GOLDEN_THRESHOLD = 10\n",
    "\n",
    "tweet_start_mask = df_final[\"tweet_volume\"] > GOLDEN_THRESHOLD\n",
    "if tweet_start_mask.any():\n",
    "    golden_start = df_final.loc[tweet_start_mask].index.min()\n",
    "    rows_before  = len(df_final)\n",
    "    df_final     = df_final.loc[golden_start:].copy()\n",
    "    rows_cropped = rows_before - len(df_final)\n",
    "    print(f\"\\n   ‚úÇÔ∏è  Golden Window (threshold > {GOLDEN_THRESHOLD}):\")\n",
    "    print(f\"       Cropped {rows_cropped:,} rows before {golden_start}\")\n",
    "    print(f\"       Dataset now starts at: {df_final.index.min()}\")\n",
    "else:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  No tweet_volume > {GOLDEN_THRESHOLD} found ‚Äî skipping Golden Window crop\")\n",
    "\n",
    "# --- Explicit dropna on critical columns -------------------------------------\n",
    "# FIX: The previous `df_final.dropna(inplace=True)` silently failed to remove\n",
    "# rows where macro columns (cpi, interest_rate) were NaN due to the 15-day\n",
    "# lag shift.  We now target the exact subset of critical columns and reassign\n",
    "# explicitly to guarantee the operation takes effect.\n",
    "critical_cols = [\"close\", \"spx_close\"]\n",
    "if \"cpi\" in df_final.columns:\n",
    "    critical_cols.append(\"cpi\")\n",
    "if \"interest_rate\" in df_final.columns:\n",
    "    critical_cols.append(\"interest_rate\")\n",
    "\n",
    "pre_drop = len(df_final)\n",
    "df_final = df_final.dropna(subset=critical_cols)\n",
    "post_drop = pre_drop - len(df_final)\n",
    "print(f\"\\n   üßπ Dropna on {critical_cols}:\")\n",
    "print(f\"       Before: {pre_drop:,} rows\")\n",
    "print(f\"       After:  {len(df_final):,} rows\")\n",
    "print(f\"       Removed: {post_drop:,} rows\")\n",
    "\n",
    "# --- Dtype optimization (float64 ‚Üí float32 saves ~50% RAM) ------------------\n",
    "float_cols = df_final.select_dtypes(include=\"float64\").columns\n",
    "df_final[float_cols] = df_final[float_cols].astype(np.float32)\n",
    "\n",
    "print(f\"\\n‚úÖ Grand Merge complete!\")\n",
    "print(f\"   Shape: {df_final.shape}\")\n",
    "print(f\"   Memory: {df_final.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n",
    "print(f\"   Date range: {df_final.index.min()} ‚Üí {df_final.index.max()}\")\n",
    "print(f\"   Columns: {list(df_final.columns)}\")\n",
    "\n",
    "# Free intermediate dataframes\n",
    "del df_sentiment_hourly, df_spx_hourly, df_dxy_hourly, df_fg_hourly\n",
    "if not df_macro_hourly.empty:\n",
    "    del df_macro_hourly\n",
    "del df_market\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfbd0f7",
   "metadata": {},
   "source": [
    "## Step 5 ‚Äî Validation & Export\n",
    "\n",
    "Final quality checks before writing the master Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cc5aab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:03:17.119891Z",
     "iopub.status.busy": "2026-02-18T10:03:17.119176Z",
     "iopub.status.idle": "2026-02-18T10:03:17.191247Z",
     "shell.execute_reply": "2026-02-18T10:03:17.190197Z",
     "shell.execute_reply.started": "2026-02-18T10:03:17.119863Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  CHIMERA 2.0 ‚Äî MASTER DATASET VALIDATION REPORT\n",
      "======================================================================\n",
      "\n",
      "üìã Schema (df_final.info):\n",
      "----------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 44112 entries, 2021-02-05 10:00:00 to 2026-02-17 23:00:00\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   open            44112 non-null  float32\n",
      " 1   high            44112 non-null  float32\n",
      " 2   low             44112 non-null  float32\n",
      " 3   close           44112 non-null  float32\n",
      " 4   volume          44112 non-null  float32\n",
      " 5   sentiment_mean  44112 non-null  float32\n",
      " 6   sentiment_std   44112 non-null  float32\n",
      " 7   tweet_volume    44112 non-null  float32\n",
      " 8   spx_close       44112 non-null  float32\n",
      " 9   dxy_close       44112 non-null  float32\n",
      " 10  fear_greed      44112 non-null  float32\n",
      " 11  cpi             44112 non-null  float32\n",
      " 12  interest_rate   44112 non-null  float32\n",
      "dtypes: float32(13)\n",
      "memory usage: 2.5 MB\n",
      "\n",
      "\n",
      "üîç Null Audit:\n",
      "----------------------------------------\n",
      "   ‚úÖ ZERO nulls across all columns!\n",
      "\n",
      "\n",
      "‚è±Ô∏è  Temporal Continuity:\n",
      "----------------------------------------\n",
      "   Expected frequency : 1H\n",
      "   Total rows         : 44,112\n",
      "   Date range         : 2021-02-05 10:00:00 ‚Üí 2026-02-17 23:00:00\n",
      "   Gaps > 1H found   : 7\n",
      "   Largest gaps:\n",
      "      2021-08-13 06:00:00  ‚Üí  gap = 0 days 05:00:00\n",
      "      2021-04-25 08:00:00  ‚Üí  gap = 0 days 04:00:00\n",
      "      2021-04-20 04:00:00  ‚Üí  gap = 0 days 03:00:00\n",
      "      2021-09-29 09:00:00  ‚Üí  gap = 0 days 03:00:00\n",
      "      2021-02-11 05:00:00  ‚Üí  gap = 0 days 02:00:00\n",
      "\n",
      "\n",
      "üìä Statistical Summary (first 5 cols):\n",
      "----------------------------------------\n",
      "             open        high         low       close      volume\n",
      "count  44112.0000  44112.0000  44112.0000  44112.0000  44112.0000\n",
      "mean   55533.5000  55745.9844  55311.8008  55534.1719   3026.7627\n",
      "std    29631.8652  29707.3516  29552.9395  29631.8125   4702.2041\n",
      "min    15648.2305  15769.9902  15476.0000  15649.5195      0.0000\n",
      "25%    29455.5000  29545.6426  29392.4297  29455.4951    719.0018\n",
      "50%    47937.5801  48183.7148  47692.1367  47940.2285   1412.5595\n",
      "75%    72155.7910  72381.7227  71840.1621  72155.7910   3166.0773\n",
      "max   126011.1797 126199.6328 125252.7422 126011.1797 137207.1875\n",
      "\n",
      "\n",
      "üëÄ Head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>sentiment_mean</th>\n",
       "      <th>sentiment_std</th>\n",
       "      <th>tweet_volume</th>\n",
       "      <th>spx_close</th>\n",
       "      <th>dxy_close</th>\n",
       "      <th>fear_greed</th>\n",
       "      <th>cpi</th>\n",
       "      <th>interest_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-05 10:00:00</th>\n",
       "      <td>37251.0703</td>\n",
       "      <td>37655.4414</td>\n",
       "      <td>37200.0000</td>\n",
       "      <td>37395.9102</td>\n",
       "      <td>2315.2617</td>\n",
       "      <td>0.3789</td>\n",
       "      <td>0.3420</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>3886.8301</td>\n",
       "      <td>129.1544</td>\n",
       "      <td>66.0000</td>\n",
       "      <td>262.6390</td>\n",
       "      <td>0.0900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-05 11:00:00</th>\n",
       "      <td>37395.9219</td>\n",
       "      <td>37733.7500</td>\n",
       "      <td>37395.7812</td>\n",
       "      <td>37691.3203</td>\n",
       "      <td>2165.1494</td>\n",
       "      <td>0.1088</td>\n",
       "      <td>0.3642</td>\n",
       "      <td>88.0000</td>\n",
       "      <td>3886.8301</td>\n",
       "      <td>129.1544</td>\n",
       "      <td>66.0000</td>\n",
       "      <td>262.6390</td>\n",
       "      <td>0.0900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-05 12:00:00</th>\n",
       "      <td>37691.3203</td>\n",
       "      <td>38151.6914</td>\n",
       "      <td>37527.1406</td>\n",
       "      <td>37850.3594</td>\n",
       "      <td>4197.9575</td>\n",
       "      <td>0.2038</td>\n",
       "      <td>0.3425</td>\n",
       "      <td>139.0000</td>\n",
       "      <td>3886.8301</td>\n",
       "      <td>129.1544</td>\n",
       "      <td>66.0000</td>\n",
       "      <td>262.6390</td>\n",
       "      <td>0.0900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          open       high        low      close    volume  \\\n",
       "timestamp                                                                   \n",
       "2021-02-05 10:00:00 37251.0703 37655.4414 37200.0000 37395.9102 2315.2617   \n",
       "2021-02-05 11:00:00 37395.9219 37733.7500 37395.7812 37691.3203 2165.1494   \n",
       "2021-02-05 12:00:00 37691.3203 38151.6914 37527.1406 37850.3594 4197.9575   \n",
       "\n",
       "                     sentiment_mean  sentiment_std  tweet_volume  spx_close  \\\n",
       "timestamp                                                                     \n",
       "2021-02-05 10:00:00          0.3789         0.3420       11.0000  3886.8301   \n",
       "2021-02-05 11:00:00          0.1088         0.3642       88.0000  3886.8301   \n",
       "2021-02-05 12:00:00          0.2038         0.3425      139.0000  3886.8301   \n",
       "\n",
       "                     dxy_close  fear_greed      cpi  interest_rate  \n",
       "timestamp                                                           \n",
       "2021-02-05 10:00:00   129.1544     66.0000 262.6390         0.0900  \n",
       "2021-02-05 11:00:00   129.1544     66.0000 262.6390         0.0900  \n",
       "2021-02-05 12:00:00   129.1544     66.0000 262.6390         0.0900  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: VALIDATION & EXPORT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"  CHIMERA 2.0 ‚Äî MASTER DATASET VALIDATION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- Schema ------------------------------------------------------------------\n",
    "print(\"\\nüìã Schema (df_final.info):\")\n",
    "print(\"-\" * 40)\n",
    "df_final.info(memory_usage=\"deep\")\n",
    "\n",
    "# --- Null audit --------------------------------------------------------------\n",
    "print(\"\\n\\nüîç Null Audit:\")\n",
    "print(\"-\" * 40)\n",
    "null_counts = df_final.isnull().sum()\n",
    "null_pct    = (df_final.isnull().mean() * 100).round(2)\n",
    "null_report = pd.DataFrame({\"nulls\": null_counts, \"pct\": null_pct})\n",
    "null_report = null_report[null_report[\"nulls\"] > 0]\n",
    "if null_report.empty:\n",
    "    print(\"   ‚úÖ ZERO nulls across all columns!\")\n",
    "else:\n",
    "    print(null_report.to_string())\n",
    "    print(f\"\\n   ‚ö† {len(null_report)} column(s) still have nulls.\")\n",
    "\n",
    "# --- Temporal continuity check -----------------------------------------------\n",
    "print(\"\\n\\n‚è±Ô∏è  Temporal Continuity:\")\n",
    "print(\"-\" * 40)\n",
    "time_diffs = df_final.index.to_series().diff().dropna()\n",
    "expected   = pd.Timedelta(hours=1)\n",
    "gaps       = time_diffs[time_diffs > expected]\n",
    "print(f\"   Expected frequency : 1H\")\n",
    "print(f\"   Total rows         : {len(df_final):,}\")\n",
    "print(f\"   Date range         : {df_final.index.min()} ‚Üí {df_final.index.max()}\")\n",
    "print(f\"   Gaps > 1H found   : {len(gaps):,}\")\n",
    "if len(gaps) > 0 and len(gaps) <= 10:\n",
    "    print(\"   Largest gaps:\")\n",
    "    for ts, gap in gaps.nlargest(5).items():\n",
    "        print(f\"      {ts}  ‚Üí  gap = {gap}\")\n",
    "\n",
    "# --- Quick statistical sanity check ------------------------------------------\n",
    "print(\"\\n\\nüìä Statistical Summary (first 5 cols):\")\n",
    "print(\"-\" * 40)\n",
    "print(df_final.describe().iloc[:, :5].to_string())\n",
    "\n",
    "# --- Preview -----------------------------------------------------------------\n",
    "print(\"\\n\\nüëÄ Head:\")\n",
    "df_final.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fab9834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:03:17.192827Z",
     "iopub.status.busy": "2026-02-18T10:03:17.192491Z",
     "iopub.status.idle": "2026-02-18T10:03:17.539079Z",
     "shell.execute_reply": "2026-02-18T10:03:17.538183Z",
     "shell.execute_reply.started": "2026-02-18T10:03:17.192799Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: /kaggle/working/chimera_master_dataset.parquet\n",
      "   Size:  1.8 MB\n",
      "   Rows:  44,112\n",
      "   Cols:  13\n",
      "\n",
      "‚úÖ Read-back verification passed ‚Äî Parquet file is healthy.\n",
      "\n",
      "üèÅ Phase 1: Data Kitchen COMPLETE. Ready for Feature Engineering.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVE TO PARQUET\n",
    "# ============================================================================\n",
    "# Parquet is columnar, compressed, and preserves dtypes ‚Äî ideal for\n",
    "# downstream feature engineering and ML pipelines.\n",
    "# ============================================================================\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_final.to_parquet(OUTPUT_FILE, engine=\"pyarrow\", index=True)\n",
    "\n",
    "file_size = OUTPUT_FILE.stat().st_size / 1e6\n",
    "print(f\"üíæ Saved: {OUTPUT_FILE}\")\n",
    "print(f\"   Size:  {file_size:.1f} MB\")\n",
    "print(f\"   Rows:  {len(df_final):,}\")\n",
    "print(f\"   Cols:  {df_final.shape[1]}\")\n",
    "\n",
    "# --- Quick read-back sanity test ---------------------------------------------\n",
    "df_check = pd.read_parquet(OUTPUT_FILE)\n",
    "assert df_check.shape == df_final.shape, \"Shape mismatch on read-back!\"\n",
    "assert (df_check.columns == df_final.columns).all(), \"Column mismatch on read-back!\"\n",
    "print(\"\\n‚úÖ Read-back verification passed ‚Äî Parquet file is healthy.\")\n",
    "print(\"\\nüèÅ Phase 1: Data Kitchen COMPLETE. Ready for Feature Engineering.\")\n",
    "del df_check\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 884064,
     "datasetId": 312039,
     "sourceId": 857568,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 15735881,
     "datasetId": 5656419,
     "sourceId": 14873510,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 5213331,
     "datasetId": 1155801,
     "sourceId": 5141688,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 13898832,
     "datasetId": 5432286,
     "sourceId": 13205834,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 13579459,
     "datasetId": 8179058,
     "sourceId": 12925742,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 12869225,
     "datasetId": 7762459,
     "sourceId": 12315111,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
