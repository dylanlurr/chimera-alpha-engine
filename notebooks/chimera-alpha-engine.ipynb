{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2e2d41",
   "metadata": {},
   "source": [
    "# ğŸ§¬ Project Chimera â€” Phase 2: TFT Training Pipeline\n",
    "\n",
    "**Temporal Fusion Transformer** for multi-asset 5-day return forecasting.\n",
    "\n",
    "| Setting | Value |\n",
    "|---|---|\n",
    "| Platform | Kaggle (GPU T4 Ã— 2) |\n",
    "| Lookback | 60 calendar days |\n",
    "| Horizon | 5 days (`Target_Return_5D`) |\n",
    "| Model | TFT via `pytorch-forecasting` |\n",
    "| Precision | FP16-mixed (halves VRAM) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61867e90",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 1A â€” The Installer (Run ONCE, then Restart Session)\n",
    "# =============================================================================\n",
    "# ROOT CAUSE: Kaggle uses Python 3.12, but pytorch-forecasting<=1.0.0\n",
    "# requires Python <3.11.  We need pytorch-forecasting>=1.1.0, which ships\n",
    "# with the unified \"lightning\" 2.x package (not the old \"pytorch-lightning\").\n",
    "\n",
    "!pip install -q \"pytorch-forecasting>=1.1.0\"\n",
    "\n",
    "# Verify what actually got installed\n",
    "!pip list | grep -iE \"forecasting|lightning\"\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… DONE. Now click 'Restart Session' in the Run menu.\")\n",
    "print(\"   After restart, SKIP this cell and run Cell 1B below.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "295eb838",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T08:14:32.379895Z",
     "iopub.status.busy": "2026-02-10T08:14:32.379668Z",
     "iopub.status.idle": "2026-02-10T08:14:39.298885Z",
     "shell.execute_reply": "2026-02-10T08:14:39.298246Z",
     "shell.execute_reply.started": "2026-02-10T08:14:32.379872Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries Loaded! Ready to train.\n",
      "   Python    : 3.12.12\n",
      "   Torch     : 2.8.0+cu126\n",
      "   Lightning : 2.6.1\n",
      "   GPU       : Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 1B â€” The Loader (Run AFTER Restart)\n",
    "# =============================================================================\n",
    "# pytorch-forecasting >=1.1 uses the unified \"lightning\" package.\n",
    "# Import path is now: lightning.pytorch  (not the old \"pytorch-lightning\").\n",
    "\n",
    "try:\n",
    "    import sys\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    from pathlib import Path\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import lightning.pytorch as pl          # â† unified Lightning 2.x\n",
    "\n",
    "    from pytorch_forecasting import (\n",
    "        TimeSeriesDataSet,\n",
    "        TemporalFusionTransformer,\n",
    "        GroupNormalizer,\n",
    "    )\n",
    "    from pytorch_forecasting.metrics import QuantileLoss\n",
    "    from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "\n",
    "    # Reproducibility\n",
    "    pl.seed_everything(42, workers=True)\n",
    "    NUM_WORKERS: int = 2\n",
    "\n",
    "    print(\"âœ… Libraries Loaded! Ready to train.\")\n",
    "    print(f\"   Python    : {sys.version.split()[0]}\")\n",
    "    print(f\"   Torch     : {torch.__version__}\")\n",
    "    print(f\"   Lightning : {pl.__version__}\")\n",
    "    print(f\"   GPU       : {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"âŒ Error: Did you run Cell 1A and Restart the Session?\")\n",
    "    print(f\"   Details: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf9a94fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T08:15:04.993843Z",
     "iopub.status.busy": "2026-02-10T08:15:04.993511Z",
     "iopub.status.idle": "2026-02-10T08:15:05.199981Z",
     "shell.execute_reply": "2026-02-10T08:15:05.199401Z",
     "shell.execute_reply.started": "2026-02-10T08:15:04.993813Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /kaggle/input/datasets/wangdylanlorrenzo/chimera-alpha-engine-aligned-market-data-v1/chimera_data_v1.csv\n",
      "Wide shape: (1649, 120)\n",
      "Date range: 2021-07-30 â†’ 2026-02-02\n",
      "Tickers detected (10): ['BTC-USD', 'COIN', 'DX-Y.NYB', 'ETH-USD', 'HOOD', 'MA', 'PYPL', 'SOL-USD', 'V', '^TNX']\n",
      "\n",
      "Long-format shape : (16490, 16)\n",
      "time_idx range    : 0 â†’ 1648\n",
      "Tickers           : 10\n",
      "Asset classes      : ['CRYPTO', 'FINTECH']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Trend_Spread</th>\n",
       "      <th>Stoch_K</th>\n",
       "      <th>Stoch_D</th>\n",
       "      <th>Is_Oversold</th>\n",
       "      <th>RVOL</th>\n",
       "      <th>Log_Returns</th>\n",
       "      <th>Target_Return_5D</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Asset_Class</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42235.546875</td>\n",
       "      <td>42235.546875</td>\n",
       "      <td>38397.355469</td>\n",
       "      <td>40027.484375</td>\n",
       "      <td>3.307278e+10</td>\n",
       "      <td>0.025948</td>\n",
       "      <td>95.263215</td>\n",
       "      <td>90.712255</td>\n",
       "      <td>0</td>\n",
       "      <td>1.299297</td>\n",
       "      <td>0.054172</td>\n",
       "      <td>-0.058909</td>\n",
       "      <td>BTC-USD</td>\n",
       "      <td>2021-07-30</td>\n",
       "      <td>CRYPTO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41626.195312</td>\n",
       "      <td>42231.449219</td>\n",
       "      <td>41110.832031</td>\n",
       "      <td>42196.304688</td>\n",
       "      <td>2.580285e+10</td>\n",
       "      <td>0.029934</td>\n",
       "      <td>96.072158</td>\n",
       "      <td>94.442837</td>\n",
       "      <td>0</td>\n",
       "      <td>1.002478</td>\n",
       "      <td>-0.014533</td>\n",
       "      <td>-0.018177</td>\n",
       "      <td>BTC-USD</td>\n",
       "      <td>2021-07-31</td>\n",
       "      <td>CRYPTO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39974.894531</td>\n",
       "      <td>42541.679688</td>\n",
       "      <td>39540.941406</td>\n",
       "      <td>41460.843750</td>\n",
       "      <td>2.668844e+10</td>\n",
       "      <td>0.031547</td>\n",
       "      <td>91.931081</td>\n",
       "      <td>94.422151</td>\n",
       "      <td>0</td>\n",
       "      <td>1.032139</td>\n",
       "      <td>-0.040478</td>\n",
       "      <td>0.071085</td>\n",
       "      <td>BTC-USD</td>\n",
       "      <td>2021-08-01</td>\n",
       "      <td>CRYPTO</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39201.945312</td>\n",
       "      <td>40419.179688</td>\n",
       "      <td>38746.347656</td>\n",
       "      <td>39907.261719</td>\n",
       "      <td>2.559527e+10</td>\n",
       "      <td>0.031026</td>\n",
       "      <td>83.485075</td>\n",
       "      <td>90.496105</td>\n",
       "      <td>0</td>\n",
       "      <td>0.977623</td>\n",
       "      <td>-0.019525</td>\n",
       "      <td>0.136571</td>\n",
       "      <td>BTC-USD</td>\n",
       "      <td>2021-08-02</td>\n",
       "      <td>CRYPTO</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38152.980469</td>\n",
       "      <td>39750.031250</td>\n",
       "      <td>37782.050781</td>\n",
       "      <td>39178.402344</td>\n",
       "      <td>2.618983e+10</td>\n",
       "      <td>0.029121</td>\n",
       "      <td>73.823057</td>\n",
       "      <td>83.079738</td>\n",
       "      <td>0</td>\n",
       "      <td>0.991221</td>\n",
       "      <td>-0.027122</td>\n",
       "      <td>0.147961</td>\n",
       "      <td>BTC-USD</td>\n",
       "      <td>2021-08-03</td>\n",
       "      <td>CRYPTO</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Close          High           Low          Open        Volume  \\\n",
       "0  42235.546875  42235.546875  38397.355469  40027.484375  3.307278e+10   \n",
       "1  41626.195312  42231.449219  41110.832031  42196.304688  2.580285e+10   \n",
       "2  39974.894531  42541.679688  39540.941406  41460.843750  2.668844e+10   \n",
       "3  39201.945312  40419.179688  38746.347656  39907.261719  2.559527e+10   \n",
       "4  38152.980469  39750.031250  37782.050781  39178.402344  2.618983e+10   \n",
       "\n",
       "   Trend_Spread    Stoch_K    Stoch_D  Is_Oversold      RVOL  Log_Returns  \\\n",
       "0      0.025948  95.263215  90.712255            0  1.299297     0.054172   \n",
       "1      0.029934  96.072158  94.442837            0  1.002478    -0.014533   \n",
       "2      0.031547  91.931081  94.422151            0  1.032139    -0.040478   \n",
       "3      0.031026  83.485075  90.496105            0  0.977623    -0.019525   \n",
       "4      0.029121  73.823057  83.079738            0  0.991221    -0.027122   \n",
       "\n",
       "   Target_Return_5D   Ticker       Date Asset_Class  time_idx  \n",
       "0         -0.058909  BTC-USD 2021-07-30      CRYPTO         0  \n",
       "1         -0.018177  BTC-USD 2021-07-31      CRYPTO         1  \n",
       "2          0.071085  BTC-USD 2021-08-01      CRYPTO         2  \n",
       "3          0.136571  BTC-USD 2021-08-02      CRYPTO         3  \n",
       "4          0.147961  BTC-USD 2021-08-03      CRYPTO         4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 2 â€” Data Loading & Preprocessing\n",
    "# =============================================================================\n",
    "# Flexible path: try Kaggle input dir first, then local fallback.\n",
    "KAGGLE_PATH = Path(\"/kaggle/input/datasets/wangdylanlorrenzo/chimera-alpha-engine-aligned-market-data-v1/chimera_data_v1.csv\")\n",
    "LOCAL_PATH  = Path(\"../data/processed/chimera_data_v1.csv\")\n",
    "\n",
    "csv_path = KAGGLE_PATH if KAGGLE_PATH.exists() else LOCAL_PATH\n",
    "print(f\"Loading data from: {csv_path.resolve()}\")\n",
    "raw_wide = pd.read_csv(csv_path, index_col=0, parse_dates=True)\n",
    "raw_wide.index.name = \"Date\"\n",
    "\n",
    "print(f\"Wide shape: {raw_wide.shape}\")\n",
    "print(f\"Date range: {raw_wide.index.min().date()} â†’ {raw_wide.index.max().date()}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Melt wide format (TICKER_Feature) â†’ long format required by TFT\n",
    "# ---------------------------------------------------------------------------\n",
    "# Columns follow the pattern: \"TICKER_Feature\"\n",
    "# Parse every column into (ticker, feature) pairs.\n",
    "FEATURES_PER_TICKER = [\n",
    "    \"Close\", \"High\", \"Low\", \"Open\", \"Volume\",\n",
    "    \"Trend_Spread\", \"Stoch_K\", \"Stoch_D\", \"Is_Oversold\",\n",
    "    \"RVOL\", \"Log_Returns\", \"Target_Return_5D\",\n",
    "]\n",
    "\n",
    "records: list[pd.DataFrame] = []\n",
    "tickers_found: list[str] = []\n",
    "\n",
    "# Derive unique tickers from columns\n",
    "col_set = set(raw_wide.columns)\n",
    "for col in sorted(raw_wide.columns):\n",
    "    # Find the feature suffix; the ticker is everything before it\n",
    "    for feat in FEATURES_PER_TICKER:\n",
    "        if col.endswith(f\"_{feat}\"):\n",
    "            ticker = col[: -(len(feat) + 1)]  # strip \"_Feature\"\n",
    "            if ticker not in tickers_found:\n",
    "                # Verify ALL features exist for this ticker\n",
    "                expected = [f\"{ticker}_{f}\" for f in FEATURES_PER_TICKER]\n",
    "                if all(e in col_set for e in expected):\n",
    "                    tickers_found.append(ticker)\n",
    "            break\n",
    "\n",
    "print(f\"Tickers detected ({len(tickers_found)}): {tickers_found}\")\n",
    "\n",
    "for ticker in tickers_found:\n",
    "    sub = raw_wide[[f\"{ticker}_{f}\" for f in FEATURES_PER_TICKER]].copy()\n",
    "    sub.columns = FEATURES_PER_TICKER  # strip ticker prefix\n",
    "    sub[\"Ticker\"] = ticker\n",
    "    sub[\"Date\"] = sub.index if isinstance(sub.index, pd.DatetimeIndex) else pd.to_datetime(sub.index)\n",
    "    records.append(sub)\n",
    "\n",
    "df = pd.concat(records, ignore_index=True)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Asset class label (static categorical for TFT)\n",
    "# ---------------------------------------------------------------------------\n",
    "CRYPTO_TICKERS: set[str] = {\"BTC-USD\", \"ETH-USD\", \"SOL-USD\"}\n",
    "df[\"Asset_Class\"] = df[\"Ticker\"].apply(\n",
    "    lambda t: \"CRYPTO\" if t in CRYPTO_TICKERS else \"FINTECH\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CRITICAL (TFT requirement): time_idx â€” continuous integer per group\n",
    "# ---------------------------------------------------------------------------\n",
    "df.sort_values([\"Ticker\", \"Date\"], inplace=True)\n",
    "df[\"time_idx\"] = df.groupby(\"Ticker\").cumcount()\n",
    "\n",
    "# Cast categoricals\n",
    "df[\"Ticker\"]      = df[\"Ticker\"].astype(str)\n",
    "df[\"Asset_Class\"] = df[\"Asset_Class\"].astype(str)\n",
    "\n",
    "# Fill any residual NaNs in numeric columns to prevent NaN loss\n",
    "numeric_cols = df.select_dtypes(include=\"number\").columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"\\nLong-format shape : {df.shape}\")\n",
    "print(f\"time_idx range    : {df['time_idx'].min()} â†’ {df['time_idx'].max()}\")\n",
    "print(f\"Tickers           : {df['Ticker'].nunique()}\")\n",
    "print(f\"Asset classes      : {df['Asset_Class'].unique().tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f6d9c96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T08:22:30.725734Z",
     "iopub.status.busy": "2026-02-10T08:22:30.724989Z",
     "iopub.status.idle": "2026-02-10T08:22:32.437027Z",
     "shell.execute_reply": "2026-02-10T08:22:32.436160Z",
     "shell.execute_reply.started": "2026-02-10T08:22:30.725688Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max time_idx     : 1648\n",
      "Training cutoff  : 1588\n",
      "Train rows (est) : 15890\n",
      "Val rows (est)   : 600\n",
      "\n",
      "âœ… TimeSeriesDataSet created â€” 15250 samples\n",
      "   Encoder length : 60\n",
      "   Prediction len : 5\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 3 â€” TimeSeriesDataSet Configuration (The Architecture)\n",
    "# =============================================================================\n",
    "# The TFT requires a structured TimeSeriesDataSet that declares the role of\n",
    "# every column: static vs. time-varying, known vs. unknown, real vs. categorical.\n",
    "\n",
    "# Training cutoff: everything except the last 60 time steps per group\n",
    "max_time_idx: int = df[\"time_idx\"].max()\n",
    "training_cutoff: int = max_time_idx - 60  # last 60 days â†’ validation\n",
    "\n",
    "print(f\"Max time_idx     : {max_time_idx}\")\n",
    "print(f\"Training cutoff  : {training_cutoff}\")\n",
    "print(f\"Train rows (est) : {(df['time_idx'] <= training_cutoff).sum()}\")\n",
    "print(f\"Val rows (est)   : {(df['time_idx'] >  training_cutoff).sum()}\")\n",
    "\n",
    "# --- Time-varying unknown reals ------------------------------------------\n",
    "# These are the Phase-1 engineered features the model will learn from.\n",
    "# NOTE: BB_Width was specified in the plan but is not present in the Phase-1\n",
    "# feature set.  We use the features that actually exist in the CSV.\n",
    "TIME_VARYING_UNKNOWN_REALS: list[str] = [\n",
    "    \"Close\",\n",
    "    \"Log_Returns\",\n",
    "    \"Trend_Spread\",\n",
    "    \"Stoch_K\",\n",
    "    \"Stoch_D\",\n",
    "    \"RVOL\",\n",
    "]\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    df[df[\"time_idx\"] <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Target_Return_5D\",\n",
    "    group_ids=[\"Ticker\"],\n",
    "    min_encoder_length=60,          # fixed 60-day lookback\n",
    "    max_encoder_length=60,\n",
    "    min_prediction_length=5,        # 5-day forecast horizon\n",
    "    max_prediction_length=5,\n",
    "    static_categoricals=[\"Asset_Class\", \"Ticker\"],\n",
    "    time_varying_unknown_reals=TIME_VARYING_UNKNOWN_REALS,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"Ticker\"],\n",
    "        transformation=\"softplus\",   # ensures positive scale parameters\n",
    "    ),\n",
    "    add_relative_time_idx=True,      # relative position inside the window\n",
    "    add_target_scales=True,          # passes (center, scale) to the model\n",
    "    add_encoder_length=True,         # lets model know how long the input is\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… TimeSeriesDataSet created â€” {len(training)} samples\")\n",
    "print(f\"   Encoder length : {training.max_encoder_length}\")\n",
    "print(f\"   Prediction len : {training.max_prediction_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e3d1e50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T08:22:35.270619Z",
     "iopub.status.busy": "2026-02-10T08:22:35.270290Z",
     "iopub.status.idle": "2026-02-10T08:22:35.336704Z",
     "shell.execute_reply": "2026-02-10T08:22:35.336063Z",
     "shell.execute_reply.started": "2026-02-10T08:22:35.270586Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches : 238\n",
      "Val batches   : 1\n",
      "Batch size    : 64 (effective 128 via grad accumulation)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 4 â€” Dataloaders\n",
    "# =============================================================================\n",
    "# Memory-safety: batch_size=64 keeps peak VRAM well within T4's 16 GB.\n",
    "BATCH_SIZE: int = 64\n",
    "\n",
    "# Validation set: the LAST 60 days held out via training_cutoff\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,\n",
    "    df,                              # full dataframe (includes val period)\n",
    "    predict=True,                    # marks this as the prediction/val split\n",
    "    stop_randomization=True,         # deterministic iteration for val\n",
    ")\n",
    "\n",
    "train_dataloader = training.to_dataloader(\n",
    "    train=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "val_dataloader = validation.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=BATCH_SIZE * 2,       # no grads â†’ can afford larger batches\n",
    "    num_workers=NUM_WORKERS,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "print(f\"Train batches : {len(train_dataloader)}\")\n",
    "print(f\"Val batches   : {len(val_dataloader)}\")\n",
    "print(f\"Batch size    : {BATCH_SIZE} (effective {BATCH_SIZE * 2} via grad accumulation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d180080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T08:22:38.568584Z",
     "iopub.status.busy": "2026-02-10T08:22:38.567960Z",
     "iopub.status.idle": "2026-02-10T08:22:38.633721Z",
     "shell.execute_reply": "2026-02-10T08:22:38.633125Z",
     "shell.execute_reply.started": "2026-02-10T08:22:38.568551Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TFT initialised â€” 293,714 trainable parameters\n",
      "   Hidden size     : 64\n",
      "   LSTM layers     : 2\n",
      "   Attention heads : 4\n",
      "   Loss            : QuantileLoss (7 quantiles)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 5 â€” TFT Model Initialization (Fix: Disable Interpretation Logging)\n",
    "# =============================================================================\n",
    "# The Temporal Fusion Transformer combines:\n",
    "#   â€¢ Variable Selection Networks  â†’ automatic feature importance\n",
    "#   â€¢ Gated Residual Networks      â†’ skip connections for depth\n",
    "#   â€¢ Multi-Head Attention         â†’ long-range temporal patterns\n",
    "#   â€¢ LSTM encoder/decoder         â†’ sequential dependencies\n",
    "#\n",
    "# QuantileLoss produces a distribution (P10, P25, P50, P75, P90, P95, P99)\n",
    "# instead of a single point estimate, giving us actionable confidence bands.\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=1e-3,\n",
    "    hidden_size=64,                  # balance between capacity and VRAM\n",
    "    lstm_layers=2,\n",
    "    dropout=0.1,\n",
    "    output_size=7,                   # 7 quantiles for QuantileLoss\n",
    "    loss=QuantileLoss(),\n",
    "    attention_head_size=4,\n",
    "    reduce_on_plateau_patience=4,    # LR scheduler patience\n",
    "\n",
    "    # CRITICAL FIX: Disable auto-logging of interpretation to prevent KeyError.\n",
    "    # pytorch-forecasting's interpretation hook looks for an 'interpretation'\n",
    "    # key in the model output dict, which isn't always present depending on\n",
    "    # the version. Setting these to 0 disables the hook entirely.\n",
    "    # We can still call tft.interpret_output() manually after training.\n",
    "    log_interval=0,\n",
    "    log_val_interval=0,\n",
    ")\n",
    "\n",
    "num_params = sum(p.numel() for p in tft.parameters() if p.requires_grad)\n",
    "print(f\"âœ… TFT initialised â€” {num_params:,} trainable parameters\")\n",
    "print(f\"   Hidden size     : 64\")\n",
    "print(f\"   LSTM layers     : 2\")\n",
    "print(f\"   Attention heads : 4\")\n",
    "print(f\"   Loss            : QuantileLoss (7 quantiles)\")\n",
    "print(f\"   Logging         : Disabled (prevents interpretation KeyError)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a982a54f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T08:27:07.256611Z",
     "iopub.status.busy": "2026-02-10T08:27:07.256049Z",
     "iopub.status.idle": "2026-02-10T08:27:07.316466Z",
     "shell.execute_reply": "2026-02-10T08:27:07.315957Z",
     "shell.execute_reply.started": "2026-02-10T08:27:07.256573Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "ğŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Trainer ready (Lightning 2.x â€” FP32)\n",
      "   Precision         : 32 (FP16 incompatible with TFT attention mask)\n",
      "   Max epochs        : 20\n",
      "   Grad accumulation : 2  (effective batch = 128)\n",
      "   Gradient clip     : 0.1\n",
      "   Early stopping    : patience=5 on val_loss\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 6 â€” Trainer (Lightning 2.x Configuration)\n",
    "# =============================================================================\n",
    "# IMPORTANT: precision=\"16-mixed\" causes RuntimeError in the TFT attention\n",
    "# mask layer because the mask bias (-1e9) overflows FP16's max (~65504).\n",
    "# We use precision=\"32\" instead. With hidden_size=64 and batch_size=64,\n",
    "# the model fits comfortably in T4's 16 GB VRAM even at full precision.\n",
    "#\n",
    "# Other OOM-prevention strategies:\n",
    "#   â€¢ accumulate_grad_batches=2  â†’ effective batch = 128\n",
    "#   â€¢ gradient_clip_val=0.1      â†’ tames spiky financial gradients\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    verbose=True,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    precision=32,                     # FP32 â€” avoids TFT attention mask overflow\n",
    "    accumulate_grad_batches=2,\n",
    "    max_epochs=20,\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop, lr_monitor],\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer ready (Lightning 2.x â€” FP32)\")\n",
    "print(f\"   Precision         : 32 (FP16 incompatible with TFT attention mask)\")\n",
    "print(f\"   Max epochs        : 20\")\n",
    "print(f\"   Grad accumulation : 2  (effective batch = {BATCH_SIZE * 2})\")\n",
    "print(f\"   Gradient clip     : 0.1\")\n",
    "print(f\"   Early stopping    : patience=5 on val_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1766e883",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T08:28:39.998572Z",
     "iopub.status.busy": "2026-02-10T08:28:39.997923Z",
     "iopub.status.idle": "2026-02-10T08:29:06.230162Z",
     "shell.execute_reply": "2026-02-10T08:29:06.229003Z",
     "shell.execute_reply.started": "2026-02-10T08:28:39.998530Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">    </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name                               </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                            </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>â”ƒ\n",
       "â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0  </span>â”‚ loss                               â”‚ QuantileLoss                    â”‚      0 â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1  </span>â”‚ logging_metrics                    â”‚ ModuleList                      â”‚      0 â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2  </span>â”‚ input_embeddings                   â”‚ MultiEmbedding                  â”‚     62 â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3  </span>â”‚ prescalers                         â”‚ ModuleDict                      â”‚    160 â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4  </span>â”‚ static_variable_selection          â”‚ VariableSelectionNetwork        â”‚  5.6 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5  </span>â”‚ encoder_variable_selection         â”‚ VariableSelectionNetwork        â”‚ 12.5 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6  </span>â”‚ decoder_variable_selection         â”‚ VariableSelectionNetwork        â”‚  1.6 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7  </span>â”‚ static_context_variable_selection  â”‚ GatedResidualNetwork            â”‚ 16.8 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8  </span>â”‚ static_context_initial_hidden_lstm â”‚ GatedResidualNetwork            â”‚ 16.8 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9  </span>â”‚ static_context_initial_cell_lstm   â”‚ GatedResidualNetwork            â”‚ 16.8 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 10 </span>â”‚ static_context_enrichment          â”‚ GatedResidualNetwork            â”‚ 16.8 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 11 </span>â”‚ lstm_encoder                       â”‚ LSTM                            â”‚ 66.6 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 12 </span>â”‚ lstm_decoder                       â”‚ LSTM                            â”‚ 66.6 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 13 </span>â”‚ post_lstm_gate_encoder             â”‚ GatedLinearUnit                 â”‚  8.3 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 14 </span>â”‚ post_lstm_add_norm_encoder         â”‚ AddNorm                         â”‚    128 â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 15 </span>â”‚ static_enrichment                  â”‚ GatedResidualNetwork            â”‚ 20.9 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 16 </span>â”‚ multihead_attn                     â”‚ InterpretableMultiHeadAttention â”‚ 10.4 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 17 </span>â”‚ post_attn_gate_norm                â”‚ GateAddNorm                     â”‚  8.4 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 18 </span>â”‚ pos_wise_ff                        â”‚ GatedResidualNetwork            â”‚ 16.8 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 19 </span>â”‚ pre_output_gate_norm               â”‚ GateAddNorm                     â”‚  8.4 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 20 </span>â”‚ output_layer                       â”‚ Linear                          â”‚    455 â”‚ eval â”‚     0 â”‚\n",
       "â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mName                              \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mType                           \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mMode\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0mâ”‚ loss                               â”‚ QuantileLoss                    â”‚      0 â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0mâ”‚ logging_metrics                    â”‚ ModuleList                      â”‚      0 â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0mâ”‚ input_embeddings                   â”‚ MultiEmbedding                  â”‚     62 â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0mâ”‚ prescalers                         â”‚ ModuleDict                      â”‚    160 â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0mâ”‚ static_variable_selection          â”‚ VariableSelectionNetwork        â”‚  5.6 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0mâ”‚ encoder_variable_selection         â”‚ VariableSelectionNetwork        â”‚ 12.5 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0mâ”‚ decoder_variable_selection         â”‚ VariableSelectionNetwork        â”‚  1.6 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0mâ”‚ static_context_variable_selection  â”‚ GatedResidualNetwork            â”‚ 16.8 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0mâ”‚ static_context_initial_hidden_lstm â”‚ GatedResidualNetwork            â”‚ 16.8 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0mâ”‚ static_context_initial_cell_lstm   â”‚ GatedResidualNetwork            â”‚ 16.8 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0mâ”‚ static_context_enrichment          â”‚ GatedResidualNetwork            â”‚ 16.8 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0mâ”‚ lstm_encoder                       â”‚ LSTM                            â”‚ 66.6 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m12\u001b[0m\u001b[2m \u001b[0mâ”‚ lstm_decoder                       â”‚ LSTM                            â”‚ 66.6 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m13\u001b[0m\u001b[2m \u001b[0mâ”‚ post_lstm_gate_encoder             â”‚ GatedLinearUnit                 â”‚  8.3 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m14\u001b[0m\u001b[2m \u001b[0mâ”‚ post_lstm_add_norm_encoder         â”‚ AddNorm                         â”‚    128 â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m15\u001b[0m\u001b[2m \u001b[0mâ”‚ static_enrichment                  â”‚ GatedResidualNetwork            â”‚ 20.9 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m16\u001b[0m\u001b[2m \u001b[0mâ”‚ multihead_attn                     â”‚ InterpretableMultiHeadAttention â”‚ 10.4 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m17\u001b[0m\u001b[2m \u001b[0mâ”‚ post_attn_gate_norm                â”‚ GateAddNorm                     â”‚  8.4 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m18\u001b[0m\u001b[2m \u001b[0mâ”‚ pos_wise_ff                        â”‚ GatedResidualNetwork            â”‚ 16.8 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m19\u001b[0m\u001b[2m \u001b[0mâ”‚ pre_output_gate_norm               â”‚ GateAddNorm                     â”‚  8.4 K â”‚ eval â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m20\u001b[0m\u001b[2m \u001b[0mâ”‚ output_layer                       â”‚ Linear                          â”‚    455 â”‚ eval â”‚     0 â”‚\n",
       "â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 293 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 293 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 0                                                                                           \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 320                                                                                          \n",
       "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 293 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 293 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 0                                                                                           \n",
       "\u001b[1mModules in eval mode\u001b[0m: 320                                                                                          \n",
       "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94bcb7a98334a8f9f9d7cec6f3864a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'interpretation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_102/3340255625.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Re-enable cuDNN for any downstream GPU ops (predictions, etc.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         )\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path, weights_only)\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected state {self.state}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py\u001b[0m in \u001b[0;36mon_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;31m# `LightningModule.on_train_epoch_end`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_train_epoch_end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitoring_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_lightning_module_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_train_epoch_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_train_epoch_end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitoring_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[LightningModule]{pl_module.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pytorch_forecasting/models/base/_base_model.py\u001b[0m in \u001b[0;36mon_train_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pytorch_forecasting/models/temporal_fusion_transformer/_tft.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \"\"\"\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_interval\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_interpretation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m     def interpret_output(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pytorch_forecasting/models/temporal_fusion_transformer/_tft.py\u001b[0m in \u001b[0;36mlog_interpretation\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m    999\u001b[0m                 \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             ).sum(0)\n\u001b[0;32m-> 1001\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"interpretation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m         }\n\u001b[1;32m   1003\u001b[0m         \u001b[0;31m# normalize attention with length histogram squared to account for:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'interpretation'"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 7 â€” Execution & Model Saving\n",
    "# =============================================================================\n",
    "# Train the TFT on all data up to the cutoff, validating on the last 60 days.\n",
    "\n",
    "# WORKAROUND: cuDNN's fused RNN kernel has a bug where it loses training-mode\n",
    "# state during backward, causing \"cudnn RNN backward can only be called in\n",
    "# training mode\". Disabling cuDNN forces PyTorch's native LSTM implementation,\n",
    "# which is slightly slower but correct.\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "trainer.fit(tft, train_dataloader, val_dataloader)\n",
    "\n",
    "# Re-enable cuDNN for any downstream GPU ops (predictions, etc.)\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# --- Load best checkpoint (lowest val_loss) ---------------------------------\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "print(f\"\\nğŸ† Best checkpoint: {best_model_path}\")\n",
    "\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "# --- Save as a portable .ckpt file for easy download from Kaggle ------------\n",
    "# IMPORTANT: We copy the FULL Lightning checkpoint (not just state_dict).\n",
    "# load_from_checkpoint() requires the full checkpoint format which includes\n",
    "# hyperparameters, optimizer state, and Lightning metadata.\n",
    "# Saving only state_dict via torch.save() would cause:\n",
    "#   KeyError: 'pytorch-lightning_version'\n",
    "import shutil\n",
    "\n",
    "SAVE_PATH = Path(\"best_tft_model.ckpt\")\n",
    "shutil.copy2(best_model_path, SAVE_PATH)\n",
    "print(f\"âœ… Model saved â†’ {SAVE_PATH.resolve()}  ({SAVE_PATH.stat().st_size / 1e6:.1f} MB)\")\n",
    "\n",
    "# --- Quick sanity check: predict on validation set --------------------------\n",
    "val_predictions = best_tft.predict(val_dataloader, mode=\"quantiles\")\n",
    "print(f\"\\nValidation predictions shape: {val_predictions.shape}\")\n",
    "print(f\"Sample quantiles (first batch, first sample):\\n{val_predictions[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9456244,
     "sourceId": 14791220,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
